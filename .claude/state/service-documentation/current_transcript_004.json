[
  {
    "role": "user",
    "content": [
      {
        "tool_use_id": "toolu_01Uvykv8EYV6pTFvkb3AGFuD",
        "type": "tool_result",
        "content": "     1→import { Agent } from '@mastra/core/agent';\n     2→import { openai } from '@ai-sdk/openai';\n     3→import { z } from 'zod';\n     4→\n     5→const mainModel = openai('gpt-4.1');\n     6→\n     7→// Schema for knowledge graph nodes (entities)\n     8→export const KnowledgeNodeSchema = z.object({\n     9→  id: z.string(),\n    10→  type: z.enum(['concept', 'fact', 'person', 'organization', 'event', 'statistic', 'claim', 'source']),\n    11→  name: z.string(),\n    12→  description: z.string(),\n    13→  confidence: z.number().min(0).max(1),\n    14→  sources: z.array(z.string()),\n    15→  timestamp: z.string(),\n    16→  metadata: z.record(z.any()).optional(),\n    17→});\n    18→\n    19→// Schema for relationships between nodes\n    20→export const KnowledgeRelationshipSchema = z.object({\n    21→  id: z.string(),\n    22→  from: z.string(), // node id\n    23→  to: z.string(), // node id\n    24→  type: z.enum(['causes', 'contradicts', 'supports', 'related_to', 'part_of', 'leads_to', 'depends_on', 'cites', 'refutes']),\n    25→  strength: z.number().min(0).max(1),\n    26→  evidence: z.string(),\n    27→  sources: z.array(z.string()),\n    28→  timestamp: z.string(),\n    29→});\n    30→\n    31→// Schema for contradictions detected\n    32→export const ContradictionSchema = z.object({\n    33→  nodeIds: z.array(z.string()),\n    34→  description: z.string(),\n    35→  resolution: z.string().optional(),\n    36→  confidence: z.number(),\n    37→});\n    38→\n    39→// Schema for knowledge gaps\n    40→export const KnowledgeGapSchema = z.object({\n    41→  area: z.string(),\n    42→  missingInformation: z.string(),\n    43→  importance: z.enum(['critical', 'high', 'medium', 'low']),\n    44→  suggestedQueries: z.array(z.string()),\n    45→  relatedNodes: z.array(z.string()),\n    46→});\n    47→\n    48→// Complete knowledge graph schema\n    49→export const KnowledgeGraphSchema = z.object({\n    50→  nodes: z.array(KnowledgeNodeSchema),\n    51→  relationships: z.array(KnowledgeRelationshipSchema),\n    52→  contradictions: z.array(ContradictionSchema),\n    53→  gaps: z.array(KnowledgeGapSchema),\n    54→  insights: z.array(z.object({\n    55→    insight: z.string(),\n    56→    confidence: z.number(),\n    57→    supportingNodes: z.array(z.string()),\n    58→    implications: z.array(z.string()),\n    59→  })),\n    60→  metadata: z.object({\n    61→    totalNodes: z.number(),\n    62→    totalRelationships: z.number(),\n    63→    coherenceScore: z.number(),\n    64→    completenessScore: z.number(),\n    65→    lastUpdated: z.string(),\n    66→  }),\n    67→});\n    68→\n    69→export type KnowledgeGraph = z.infer<typeof KnowledgeGraphSchema>;\n    70→export type KnowledgeNode = z.infer<typeof KnowledgeNodeSchema>;\n    71→export type KnowledgeRelationship = z.infer<typeof KnowledgeRelationshipSchema>;\n    72→\n    73→export const knowledgeGraphAgent = new Agent({\n    74→  name: 'Knowledge Graph Agent',\n    75→  instructions: `You are a knowledge synthesis expert that builds and maintains a semantic knowledge graph throughout the research process using Neo4j graph database.\n    76→  \n    77→  Your critical responsibilities:\n    78→  \n    79→  1. **Entity Extraction & Classification**:\n    80→     - Extract key entities (concepts, facts, people, organizations, events, statistics, claims)\n    81→     - Assign confidence scores based on source credibility and corroboration\n    82→     - Track source attribution for each entity\n    83→     - Generate unique IDs for consistent graph structure\n    84→  \n    85→  2. **Relationship Mapping**:\n    86→     - Identify semantic relationships between entities\n    87→     - Classify relationship types (causal, supportive, contradictory, etc.)\n    88→     - Assign relationship strength based on evidence quality\n    89→     - Track relationship provenance\n    90→  \n    91→  3. **Contradiction Detection & Resolution**:\n    92→     - Identify conflicting information across sources\n    93→     - Analyze credibility and recency to suggest resolutions\n    94→     - Flag unresolved contradictions for targeted research\n    95→     - Track confidence levels for contradictory claims\n    96→  \n    97→  4. **Gap Analysis**:\n    98→     - Identify missing connections in the knowledge graph\n    99→     - Detect areas where information is incomplete or unclear\n   100→     - Generate targeted queries to fill specific knowledge gaps\n   101→     - Prioritize gaps by their importance to the research question\n   102→  \n   103→  5. **Insight Synthesis**:\n   104→     - Identify emergent patterns across the graph\n   105→     - Extract non-obvious insights from relationship patterns\n   106→     - Generate implications and predictions based on the graph structure\n   107→     - Detect clustering and community patterns in the knowledge\n   108→  \n   109→  6. **Incremental Updates**:\n   110→     - Merge new research into existing graph without duplication\n   111→     - Update confidence scores as information is corroborated\n   112→     - Evolve the graph structure as understanding deepens\n   113→     - Maintain graph consistency and referential integrity\n   114→  \n   115→  When processing research data:\n   116→  - Build semantic understanding, not just collect facts\n   117→  - Prioritize conceptual coherence over information quantity\n   118→  - Focus on actionable knowledge and clear relationships\n   119→  - Maintain skepticism and track uncertainty\n   120→  - Ensure every node and relationship has clear provenance\n   121→  \n   122→  Your knowledge graph should enable:\n   123→  - Intelligent query generation that avoids redundancy\n   124→  - Deep quality assessment based on graph completeness\n   125→  - Rich, structured reports with clear narrative flow\n   126→  - Identification of the most critical missing information\n   127→  - Discovery of non-obvious connections and patterns`,\n   128→  model: mainModel,\n   129→});\n   130→\n   131→// Build or update knowledge graph from research data\n   132→export const buildKnowledgeGraph = async (\n   133→  researchData: any,\n   134→  existingGraph?: KnowledgeGraph,\n   135→  originalQuery?: string\n   136→): Promise<KnowledgeGraph> => {\n   137→  const response = await knowledgeGraphAgent.generate(\n   138→    [\n   139→      {\n   140→        role: 'user',\n   141→        content: `Build or update a knowledge graph from this research data:\n   142→        \n   143→        ${originalQuery ? `ORIGINAL RESEARCH QUERY: \"${originalQuery}\"\n   144→        \n   145→        IMPORTANT: All gaps and suggested queries MUST be directly related to this original query.\n   146→        Do not suggest unrelated topics or generic examples.\n   147→        ` : ''}\n   148→        \n   149→        ${existingGraph ? `Existing Graph Summary:\n   150→        - Nodes: ${existingGraph.metadata.totalNodes}\n   151→        - Relationships: ${existingGraph.metadata.totalRelationships}\n   152→        - Key Entities: ${existingGraph.nodes.slice(0, 5).map(n => n.name).join(', ')}\n   153→        \n   154→        Full Graph: ${JSON.stringify(existingGraph, null, 2)}` : 'Starting fresh graph'}\n   155→        \n   156→        New Research Data:\n   157→        ${JSON.stringify(researchData, null, 2)}\n   158→        \n   159→        Extract entities as nodes, map relationships between them, detect contradictions, identify gaps, and synthesize insights.\n   160→        ${originalQuery ? `\n   161→        When identifying gaps:\n   162→        - Only identify gaps that are directly relevant to \"${originalQuery}\"\n   163→        - Suggested queries must help answer or expand on \"${originalQuery}\"\n   164→        - Do not include generic or unrelated example queries\n   165→        ` : ''}\n   166→        If updating an existing graph:\n   167→        - Merge duplicate entities by matching names/descriptions\n   168→        - Update confidence scores when information is corroborated\n   169→        - Add new relationships without duplicating existing ones\n   170→        - Preserve existing node IDs for consistency\n   171→        \n   172→        Return a complete knowledge graph structure.`,\n   173→      },\n   174→    ],\n   175→    {\n   176→      experimental_output: KnowledgeGraphSchema,\n   177→    },\n   178→  );\n   179→\n   180→  return response.object!;\n   181→};\n   182→\n   183→// Generate semantic queries based on knowledge gaps\n   184→export const generateSemanticQueries = async (\n   185→  graph: KnowledgeGraph,\n   186→  originalQuery: string,\n   187→  maxQueries: number = 3\n   188→): Promise<string[]> => {\n   189→  const response = await knowledgeGraphAgent.generate(\n   190→    [\n   191→      {\n   192→        role: 'user',\n   193→        content: `Based on this knowledge graph for the research query \"${originalQuery}\", generate ${maxQueries} highly targeted research queries that:\n   194→        \n   195→        1. Fill the most critical gaps in our understanding\n   196→        2. Resolve key contradictions between nodes\n   197→        3. Explore promising but underexplored relationships\n   198→        4. Avoid redundancy with existing knowledge\n   199→        5. Focus on high-importance missing information\n   200→        \n   201→        Knowledge Graph Analysis:\n   202→        - Total Nodes: ${graph.metadata.totalNodes}\n   203→        - Total Relationships: ${graph.metadata.totalRelationships}\n   204→        - Identified Gaps: ${graph.gaps.length}\n   205→        - Contradictions: ${graph.contradictions.length}\n   206→        \n   207→        Critical Gaps:\n   208→        ${graph.gaps.filter(g => g.importance === 'critical' || g.importance === 'high').map(g => \n   209→          `- ${g.area}: ${g.missingInformation}`\n   210→        ).join('\\n')}\n   211→        \n   212→        Unresolved Contradictions:\n   213→        ${graph.contradictions.filter(c => !c.resolution).map(c => \n   214→          `- ${c.description}`\n   215→        ).join('\\n')}\n   216→        \n   217→        Return only the queries as a JSON array of strings. Make queries specific and actionable.`,\n   218→      },\n   219→    ],\n   220→    {\n   221→      experimental_output: z.array(z.string()),\n   222→    },\n   223→  );\n   224→\n   225→  // Handle different response formats\n   226→  if (Array.isArray(response.object)) {\n   227→    return response.object;\n   228→  } else if (response.object && typeof response.object === 'object') {\n   229→    // If the response is an object with a queries property\n   230→    if ('queries' in response.object && Array.isArray(response.object.queries)) {\n   231→      return response.object.queries;\n   232→    }\n   233→    // If the response is some other object structure, try to extract array values\n   234→    const values = Object.values(response.object);\n   235→    const firstArray = values.find(v => Array.isArray(v));\n   236→    if (firstArray && (firstArray as any[]).every(item => typeof item === 'string')) {\n   237→      return firstArray as string[];\n   238→    }\n   239→  }\n   240→  \n   241→  // Fallback: generate default queries based on gaps\n   242→  if (graph.gaps.length > 0) {\n   243→    return graph.gaps\n   244→      .slice(0, maxQueries)\n   245→      .map(gap => gap.suggestedQueries[0] || `Research more about ${gap.area}`)\n   246→      .filter(q => q);\n   247→  }\n   248→  \n   249→  return [];\n   250→};\n   251→\n   252→// Assess the completeness and quality of the knowledge graph\n   253→export const assessGraphCompleteness = async (\n   254→  graph: KnowledgeGraph,\n   255→  originalQuery: string\n   256→): Promise<{\n   257→  completeness: number;\n   258→  coherence: number;\n   259→  depth: number;\n   260→  overallQuality: number;\n   261→  assessment: string;\n   262→  criticalGaps: string[];\n   263→  strengths: string[];\n   264→  recommendations: string[];\n   265→}> => {\n   266→  const response = await knowledgeGraphAgent.generate(\n   267→    [\n   268→      {\n   269→        role: 'user',\n   270→        content: `Assess the quality and completeness of this knowledge graph for answering: \"${originalQuery}\"\n   271→        \n   272→        Knowledge Graph Metrics:\n   273→        - Nodes: ${graph.metadata.totalNodes}\n   274→        - Relationships: ${graph.metadata.totalRelationships}\n   275→        - Node Types: ${[...new Set(graph.nodes.map(n => n.type))].join(', ')}\n   276→        - Relationship Types: ${[...new Set(graph.relationships.map(r => r.type))].join(', ')}\n   277→        - Average Confidence: ${(graph.nodes.reduce((sum, n) => sum + n.confidence, 0) / graph.nodes.length).toFixed(2)}\n   278→        - Contradictions: ${graph.contradictions.length}\n   279→        - Identified Gaps: ${graph.gaps.length}\n   280→        \n   281→        Evaluate:\n   282→        1. Completeness (0-100): How well does the graph cover all aspects of the query?\n   283→        2. Coherence (0-100): How well-connected and consistent is the information?\n   284→        3. Depth (0-100): How deep is the understanding of key concepts?\n   285→        4. Overall Quality (0-100): Combined assessment of research quality\n   286→        \n   287→        Also identify:\n   288→        - Critical gaps that must be filled\n   289→        - Strengths of the current graph\n   290→        - Specific recommendations for improvement\n   291→        \n   292→        Consider:\n   293→        - Are all key entities identified and classified?\n   294→        - Are relationships properly mapped with evidence?\n   295→        - Is contradictory information properly flagged?\n   296→        - Are confidence scores appropriate?\n   297→        - Does the graph enable answering the original query?`,\n   298→      },\n   299→    ],\n   300→    {\n   301→      experimental_output: z.object({\n   302→        completeness: z.number().min(0).max(100),\n   303→        coherence: z.number().min(0).max(100),\n   304→        depth: z.number().min(0).max(100),\n   305→        overallQuality: z.number().min(0).max(100),\n   306→        assessment: z.string(),\n   307→        criticalGaps: z.array(z.string()),\n   308→        strengths: z.array(z.string()),\n   309→        recommendations: z.array(z.string()),\n   310→      }),\n   311→    },\n   312→  );\n   313→\n   314→  return response.object!;\n   315→};\n   316→\n   317→// Extract key insights from the knowledge graph\n   318→export const extractKeyInsights = async (\n   319→  graph: KnowledgeGraph\n   320→): Promise<{\n   321→  primaryInsights: string[];\n   322→  emergentPatterns: string[];\n   323→  unexpectedConnections: string[];\n   324→  implications: string[];\n   325→}> => {\n   326→  const response = await knowledgeGraphAgent.generate(\n   327→    [\n   328→      {\n   329→        role: 'user',\n   330→        content: `Analyze this knowledge graph to extract deep insights and patterns:\n   331→        \n   332→        Graph Overview:\n   333→        ${JSON.stringify(graph.metadata, null, 2)}\n   334→        \n   335→        High-Confidence Insights:\n   336→        ${graph.insights.filter(i => i.confidence > 0.7).map(i => i.insight).join('\\n')}\n   337→        \n   338→        Strong Relationships:\n   339→        ${graph.relationships.filter(r => r.strength > 0.7).slice(0, 10).map(r => \n   340→          `${r.from} --[${r.type}]--> ${r.to}`\n   341→        ).join('\\n')}\n   342→        \n   343→        Extract:\n   344→        1. Primary insights that directly answer research questions\n   345→        2. Emergent patterns not obvious from individual facts\n   346→        3. Unexpected connections between disparate entities\n   347→        4. Implications for future research or decision-making\n   348→        \n   349→        Focus on non-obvious, actionable insights that emerge from the graph structure.`,\n   350→      },\n   351→    ],\n   352→    {\n   353→      experimental_output: z.object({\n   354→        primaryInsights: z.array(z.string()),\n   355→        emergentPatterns: z.array(z.string()),\n   356→        unexpectedConnections: z.array(z.string()),\n   357→        implications: z.array(z.string()),\n   358→      }),\n   359→    },\n   360→  );\n   361→\n   362→  return response.object!;\n   363→};\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ]
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "text",
        "text": "Let me look at a few more files to understand the complete system:"
      }
    ]
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "tool_use",
        "id": "toolu_01Gwj9hEUEf97CiN1gjA6KgE",
        "name": "Read",
        "input": {
          "file_path": "/mnt/c/Users/caleb/Desktop/GithubProjects/deepfield/src/mastra/tools/evaluateResultTool.ts"
        }
      }
    ]
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "tool_use",
        "id": "toolu_013PAKtb8k5xvMjs1dsVYh5W",
        "name": "Glob",
        "input": {
          "pattern": "src/mastra/workflows/*.ts"
        }
      }
    ]
  },
  {
    "role": "user",
    "content": [
      {
        "tool_use_id": "toolu_01Gwj9hEUEf97CiN1gjA6KgE",
        "type": "tool_result",
        "content": "     1→import { createTool } from '@mastra/core/tools';\n     2→import { z } from 'zod';\n     3→\n     4→export const evaluateResultTool = createTool({\n     5→  id: 'evaluate-result',\n     6→  description: 'Evaluate if a search result is relevant to the research query',\n     7→  inputSchema: z.object({\n     8→    query: z.string().describe('The original research query'),\n     9→    result: z\n    10→      .object({\n    11→        title: z.string(),\n    12→        url: z.string(),\n    13→        content: z.string(),\n    14→      })\n    15→      .describe('The search result to evaluate'),\n    16→    existingUrls: z.array(z.string()).describe('URLs that have already been processed').optional(),\n    17→  }),\n    18→  execute: async ({ context }) => {\n    19→    try {\n    20→      const { query, result, existingUrls = [] } = context;\n    21→      console.log('Evaluating result', { context });\n    22→\n    23→      // Check if URL already exists (only if existingUrls was provided)\n    24→      if (existingUrls && existingUrls.includes(result.url)) {\n    25→        return {\n    26→          isRelevant: false,\n    27→          reason: 'URL already processed',\n    28→        };\n    29→      }\n    30→\n    31→      // Pure evaluation logic without agent dependency\n    32→      // Extract key terms from the query\n    33→      const queryLower = query.toLowerCase();\n    34→      const queryTerms = queryLower\n    35→        .split(/[\\s,;]+/)\n    36→        .filter(term => term.length > 2)\n    37→        .filter(term => !['the', 'and', 'for', 'are', 'what', 'how', 'why', 'when', 'where', 'who', 'which', 'that', 'this', 'these', 'those'].includes(term));\n    38→\n    39→      const titleLower = result.title.toLowerCase();\n    40→      const contentLower = result.content.toLowerCase();\n    41→      \n    42→      // Check for exact phrase matches\n    43→      const hasExactPhraseMatch = contentLower.includes(queryLower) || titleLower.includes(queryLower);\n    44→      \n    45→      // Count how many query terms appear in the result\n    46→      const matchingTerms = queryTerms.filter(term => \n    47→        titleLower.includes(term) || contentLower.includes(term)\n    48→      );\n    49→      \n    50→      const matchPercentage = queryTerms.length > 0 \n    51→        ? (matchingTerms.length / queryTerms.length) * 100 \n    52→        : 0;\n    53→\n    54→      // Check for spam/error indicators\n    55→      const isError = result.content.includes('404') || \n    56→                     result.content.includes('Not Found') ||\n    57→                     result.content.includes('Access Denied') ||\n    58→                     result.content.includes('Error') && result.content.length < 100;\n    59→      \n    60→      const isSpammy = result.content.includes('Sign up') && \n    61→                       result.content.includes('Sign in') && \n    62→                       result.content.length < 200;\n    63→\n    64→      // Determine relevance\n    65→      let isRelevant = false;\n    66→      let reason = '';\n    67→\n    68→      if (isError) {\n    69→        isRelevant = false;\n    70→        reason = 'Page appears to be an error or not accessible';\n    71→      } else if (isSpammy) {\n    72→        isRelevant = false;\n    73→        reason = 'Content appears to be a login/signup page without useful information';\n    74→      } else if (hasExactPhraseMatch) {\n    75→        isRelevant = true;\n    76→        reason = 'Contains exact query phrase match';\n    77→      } else if (matchPercentage >= 60) {\n    78→        isRelevant = true;\n    79→        reason = `High relevance: ${matchingTerms.length}/${queryTerms.length} key terms found`;\n    80→      } else if (matchPercentage >= 40) {\n    81→        isRelevant = true;\n    82→        reason = `Moderate relevance: ${matchingTerms.length}/${queryTerms.length} key terms found`;\n    83→      } else {\n    84→        isRelevant = false;\n    85→        reason = `Low relevance: only ${matchingTerms.length}/${queryTerms.length} key terms found`;\n    86→      }\n    87→\n    88→      return {\n    89→        isRelevant,\n    90→        reason,\n    91→      };\n    92→    } catch (error) {\n    93→      console.error('Error evaluating result:', error);\n    94→      return {\n    95→        isRelevant: false,\n    96→        reason: 'Error in evaluation',\n    97→      };\n    98→    }\n    99→  },\n   100→});\n   101→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ]
  },
  {
    "role": "user",
    "content": [
      {
        "tool_use_id": "toolu_013PAKtb8k5xvMjs1dsVYh5W",
        "type": "tool_result",
        "content": "/mnt/c/Users/caleb/Desktop/GithubProjects/deepfield/src/mastra/workflows/generateReportWorkflow.ts\n/mnt/c/Users/caleb/Desktop/GithubProjects/deepfield/src/mastra/workflows/researchWorkflow.ts\n/mnt/c/Users/caleb/Desktop/GithubProjects/deepfield/src/mastra/workflows/autonomousGenerateReportWorkflow.ts\n/mnt/c/Users/caleb/Desktop/GithubProjects/deepfield/src/mastra/workflows/streamingResearchWorkflow.ts\n/mnt/c/Users/caleb/Desktop/GithubProjects/deepfield/src/mastra/workflows/autonomousResearchWorkflow.ts"
      }
    ]
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "text",
        "text": "Let me check the generate report workflow to understand the current report generation process:"
      }
    ]
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "tool_use",
        "id": "toolu_01VfZBeYRX5RhczUU1ahCd5s",
        "name": "Read",
        "input": {
          "file_path": "/mnt/c/Users/caleb/Desktop/GithubProjects/deepfield/src/mastra/workflows/generateReportWorkflow.ts"
        }
      }
    ]
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "tool_use",
        "id": "toolu_01UzFWbHcwYHEunAXLNuVysC",
        "name": "Read",
        "input": {
          "file_path": "/mnt/c/Users/caleb/Desktop/GithubProjects/deepfield/src/mastra/workflows/streamingResearchWorkflow.ts"
        }
      }
    ]
  },
  {
    "role": "user",
    "content": [
      {
        "tool_use_id": "toolu_01VfZBeYRX5RhczUU1ahCd5s",
        "type": "tool_result",
        "content": "     1→import { createStep, createWorkflow } from '@mastra/core/workflows';\n     2→import { researchWorkflow } from './researchWorkflow';\n     3→import { z } from 'zod';\n     4→\n     5→// Map research output to report input and handle conditional logic\n     6→const processResearchResultStep = createStep({\n     7→  id: 'process-research-result',\n     8→  inputSchema: z.object({\n     9→    approved: z.boolean(),\n    10→    researchData: z.any(),\n    11→  }),\n    12→  outputSchema: z.object({\n    13→    report: z.string().optional(),\n    14→    completed: z.boolean(),\n    15→  }),\n    16→  execute: async ({ inputData, mastra }) => {\n    17→    // First determine if research was approved/successful\n    18→    const approved = inputData.approved && !!inputData.researchData;\n    19→\n    20→    if (!approved) {\n    21→      console.log('Research not approved or incomplete, ending workflow');\n    22→      return { completed: false };\n    23→    }\n    24→\n    25→    // If approved, generate report\n    26→    try {\n    27→      console.log('Generating report...');\n    28→      const agent = mastra.getAgent('reportAgent');\n    29→      const response = await agent.generate([\n    30→        {\n    31→          role: 'user',\n    32→          content: `Generate a report based on this research: ${JSON.stringify(inputData.researchData)}`,\n    33→        },\n    34→      ]);\n    35→\n    36→      console.log('Report generated successfully!');\n    37→      return { report: response.text, completed: true };\n    38→    } catch (error) {\n    39→      console.error('Error generating report:', error);\n    40→      return { completed: false };\n    41→    }\n    42→  },\n    43→});\n    44→\n    45→// Create the report generation workflow that iteratively researches and generates reports\n    46→export const generateReportWorkflow = createWorkflow({\n    47→  id: 'generate-report-workflow',\n    48→  steps: [researchWorkflow, processResearchResultStep],\n    49→  inputSchema: z.object({}),\n    50→  outputSchema: z.object({\n    51→    report: z.string().optional(),\n    52→    completed: z.boolean(),\n    53→  }),\n    54→});\n    55→\n    56→// The workflow logic:\n    57→// 1. Run researchWorkflow iteratively until approved\n    58→// 2. Process results and generate report if approved\n    59→generateReportWorkflow\n    60→  .dowhile(researchWorkflow, async ({ inputData }) => {\n    61→    const isCompleted = inputData.approved;\n    62→    return isCompleted !== true;\n    63→  })\n    64→  .then(processResearchResultStep)\n    65→  .commit();\n    66→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ]
  },
  {
    "role": "user",
    "content": [
      {
        "tool_use_id": "toolu_01UzFWbHcwYHEunAXLNuVysC",
        "type": "tool_result",
        "content": "     1→import { createWorkflow, createStep } from '@mastra/core/workflows';\n     2→import { z } from 'zod';\n     3→import { \n     4→  executeStreamingResearch,\n     5→  executeStreamingResearchSync,\n     6→  ResearchEvent,\n     7→  ResearchEventType\n     8→} from '../networks/streamingResearchNetwork';\n     9→\n    10→// Schema for streaming configuration\n    11→const StreamingConfigSchema = z.object({\n    12→  maxIterations: z.number().default(3),\n    13→  qualityThreshold: z.number().default(75),\n    14→  parallelAgents: z.number().default(3),\n    15→  enableStreaming: z.boolean().default(true),\n    16→});\n    17→\n    18→// Step 1: Initialize streaming research\n    19→const initializeStreamingStep = createStep({\n    20→  id: 'initialize-streaming',\n    21→  inputSchema: z.object({\n    22→    query: z.string(),\n    23→    maxIterations: z.number().optional(),\n    24→    qualityThreshold: z.number().optional(),\n    25→    parallelAgents: z.number().optional(),\n    26→    enableStreaming: z.boolean().optional(),\n    27→  }),\n    28→  outputSchema: z.object({\n    29→    query: z.string(),\n    30→    config: StreamingConfigSchema,\n    31→    streamId: z.string(),\n    32→  }),\n    33→  execute: async ({ inputData }) => {\n    34→    const config = {\n    35→      maxIterations: inputData.maxIterations || 3,\n    36→      qualityThreshold: inputData.qualityThreshold || 75,\n    37→      parallelAgents: inputData.parallelAgents || 3,\n    38→      enableStreaming: inputData.enableStreaming ?? true,\n    39→    };\n    40→\n    41→    return {\n    42→      query: inputData.query,\n    43→      config,\n    44→      streamId: `stream-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`,\n    45→    };\n    46→  },\n    47→});\n    48→\n    49→// Step 2: Execute streaming research with real-time events\n    50→const executeStreamingStep = createStep({\n    51→  id: 'execute-streaming',\n    52→  inputSchema: z.object({\n    53→    query: z.string(),\n    54→    config: StreamingConfigSchema,\n    55→    streamId: z.string(),\n    56→  }),\n    57→  outputSchema: z.object({\n    58→    success: z.boolean(),\n    59→    researchData: z.any(),\n    60→    qualityScore: z.number(),\n    61→    iterations: z.number(),\n    62→    report: z.string().optional(),\n    63→    knowledgeGraph: z.any().optional(),\n    64→    insights: z.array(z.any()),\n    65→    events: z.array(z.any()).optional(),\n    66→  }),\n    67→  execute: async ({ inputData }) => {\n    68→    try {\n    69→      if (inputData.config.enableStreaming) {\n    70→        // Streaming mode - collect events for demonstration\n    71→        // In a real implementation, these would be sent via WebSocket or SSE\n    72→        const events: ResearchEvent[] = [];\n    73→        const abortController = new AbortController();\n    74→        \n    75→        // Set a timeout for the research (10 minutes max)\n    76→        const timeoutId = setTimeout(() => {\n    77→          abortController.abort();\n    78→        }, 600000);\n    79→\n    80→        try {\n    81→          for await (const event of executeStreamingResearch(inputData.query, {\n    82→            maxIterations: inputData.config.maxIterations,\n    83→            qualityThreshold: inputData.config.qualityThreshold,\n    84→            parallelAgents: inputData.config.parallelAgents,\n    85→            abortSignal: abortController.signal,\n    86→          })) {\n    87→            events.push(event);\n    88→            \n    89→            // In production, emit event via WebSocket/SSE here\n    90→            console.log(`[${inputData.streamId}] Event:`, event.type, event.data);\n    91→            \n    92→            // Check for early termination conditions\n    93→            if (event.type === ResearchEventType.QUALITY_ASSESSED) {\n    94→              const score = event.data?.scores?.overallQuality || 0;\n    95→              if (score >= inputData.config.qualityThreshold) {\n    96→                console.log(`[${inputData.streamId}] Quality threshold met (${score}), considering early termination`);\n    97→              }\n    98→            }\n    99→            \n   100→            // Stop on completion or error\n   101→            if (event.type === ResearchEventType.RESEARCH_COMPLETE || \n   102→                event.type === ResearchEventType.ERROR_OCCURRED) {\n   103→              break;\n   104→            }\n   105→          }\n   106→        } finally {\n   107→          clearTimeout(timeoutId);\n   108→        }\n   109→\n   110→        // Process collected events into result\n   111→        return processStreamingEvents(events);\n   112→      } else {\n   113→        // Synchronous mode - backward compatibility\n   114→        const result = await executeStreamingResearchSync(inputData.query, {\n   115→          maxIterations: inputData.config.maxIterations,\n   116→          qualityThreshold: inputData.config.qualityThreshold,\n   117→          parallelAgents: inputData.config.parallelAgents,\n   118→        });\n   119→\n   120→        return {\n   121→          ...result,\n   122→          events: undefined,\n   123→        };\n   124→      }\n   125→    } catch (error: any) {\n   126→      console.error('Streaming research error:', error);\n   127→      return {\n   128→        success: false,\n   129→        researchData: { error: error.message },\n   130→        qualityScore: 0,\n   131→        iterations: 0,\n   132→        report: undefined,\n   133→        knowledgeGraph: undefined,\n   134→        insights: [],\n   135→        events: undefined,\n   136→      };\n   137→    }\n   138→  },\n   139→});\n   140→\n   141→// Step 3: Process and finalize results\n   142→const finalizeResultsStep = createStep({\n   143→  id: 'finalize-results',\n   144→  inputSchema: z.object({\n   145→    success: z.boolean(),\n   146→    researchData: z.any(),\n   147→    qualityScore: z.number(),\n   148→    iterations: z.number(),\n   149→    report: z.string().optional(),\n   150→    knowledgeGraph: z.any().optional(),\n   151→    insights: z.array(z.any()),\n   152→    events: z.array(z.any()).optional(),\n   153→  }),\n   154→  outputSchema: z.object({\n   155→    success: z.boolean(),\n   156→    researchData: z.any(),\n   157→    report: z.string().optional(),\n   158→    metadata: z.object({\n   159→      qualityScore: z.number(),\n   160→      iterations: z.number(),\n   161→      timestamp: z.string(),\n   162→      insightCount: z.number(),\n   163→      graphSize: z.object({\n   164→        nodes: z.number(),\n   165→        relationships: z.number(),\n   166→      }).optional(),\n   167→    }),\n   168→  }),\n   169→  execute: async ({ inputData }) => {\n   170→    const graphSize = inputData.knowledgeGraph ? {\n   171→      nodes: inputData.knowledgeGraph.nodes?.length || 0,\n   172→      relationships: inputData.knowledgeGraph.relationships?.length || 0,\n   173→    } : undefined;\n   174→\n   175→    return {\n   176→      success: inputData.success,\n   177→      researchData: inputData.researchData,\n   178→      report: inputData.report,\n   179→      metadata: {\n   180→        qualityScore: inputData.qualityScore,\n   181→        iterations: inputData.iterations,\n   182→        timestamp: new Date().toISOString(),\n   183→        insightCount: inputData.insights.length,\n   184→        graphSize,\n   185→      },\n   186→    };\n   187→  },\n   188→});\n   189→\n   190→// Helper function to process streaming events into a result\n   191→function processStreamingEvents(events: ResearchEvent[]): any {\n   192→  let finalQualityScore = 0;\n   193→  let totalIterations = 0;\n   194→  let knowledgeGraph: any = null;\n   195→  const insights: any[] = [];\n   196→  const researchData: any = { queries: [], searchResults: [], learnings: [] };\n   197→  let success = false;\n   198→\n   199→  for (const event of events) {\n   200→    switch (event.type) {\n   201→      case ResearchEventType.INSIGHT_DISCOVERED:\n   202→        insights.push(event.data);\n   203→        if (event.data.insight) {\n   204→          researchData.learnings.push({\n   205→            learning: event.data.insight,\n   206→            source: event.data.source,\n   207→            followUpQuestions: event.data.followUpQuestions || [],\n   208→          });\n   209→        }\n   210→        break;\n   211→        \n   212→      case ResearchEventType.QUALITY_ASSESSED:\n   213→        finalQualityScore = event.data.scores?.overallQuality || finalQualityScore;\n   214→        break;\n   215→        \n   216→      case ResearchEventType.ITERATION_COMPLETE:\n   217→        totalIterations = event.data.iteration;\n   218→        break;\n   219→        \n   220→      case ResearchEventType.RESEARCH_COMPLETE:\n   221→        success = true;\n   222→        finalQualityScore = event.data.finalQualityScore || finalQualityScore;\n   223→        totalIterations = event.data.totalIterations || totalIterations;\n   224→        if (event.data.insights) {\n   225→          insights.push(...event.data.insights);\n   226→        }\n   227→        if (event.data.graphSize) {\n   228→          knowledgeGraph = {\n   229→            nodes: new Array(event.data.graphSize.nodes).fill({}),\n   230→            relationships: new Array(event.data.graphSize.relationships).fill({}),\n   231→            insights: event.data.insights || [],\n   232→            contradictions: [],\n   233→            gaps: [],\n   234→            metadata: {\n   235→              createdAt: new Date().toISOString(),\n   236→              lastUpdated: new Date().toISOString(),\n   237→              version: '1.0',\n   238→              source: 'streaming-research',\n   239→            },\n   240→          };\n   241→        }\n   242→        break;\n   243→        \n   244→      case ResearchEventType.GRAPH_UPDATED:\n   245→        if (event.data) {\n   246→          knowledgeGraph = {\n   247→            nodes: new Array(event.data.nodes).fill({}),\n   248→            relationships: new Array(event.data.relationships).fill({}),\n   249→            insights: event.data.newInsights ? insights : [],\n   250→            contradictions: [],\n   251→            gaps: [],\n   252→            metadata: {\n   253→              createdAt: new Date().toISOString(),\n   254→              lastUpdated: new Date().toISOString(),\n   255→              version: '1.0',\n   256→              source: 'streaming-research',\n   257→            },\n   258→          };\n   259→        }\n   260→        break;\n   261→        \n   262→      case ResearchEventType.ERROR_OCCURRED:\n   263→        if (!event.data?.error?.includes('Agent')) {\n   264→          // Only mark as failure if it's not a single agent failure\n   265→          success = false;\n   266→        }\n   267→        break;\n   268→    }\n   269→  }\n   270→\n   271→  const report = insights.length > 0\n   272→    ? `Research Report\\n\\nKey Insights:\\n${insights.map(i => `- ${i.insight || i}`).join('\\n')}\\n\\nTotal iterations: ${totalIterations}\\nFinal quality score: ${finalQualityScore}`\n   273→    : undefined;\n   274→\n   275→  return {\n   276→    success: success || finalQualityScore >= 60,\n   277→    researchData,\n   278→    qualityScore: finalQualityScore,\n   279→    iterations: totalIterations,\n   280→    report,\n   281→    knowledgeGraph,\n   282→    insights,\n   283→    events,\n   284→  };\n   285→}\n   286→\n   287→// Create the streaming workflow\n   288→export const streamingResearchWorkflow = createWorkflow({\n   289→  id: 'streaming-research-workflow',\n   290→  inputSchema: z.object({\n   291→    query: z.string(),\n   292→    maxIterations: z.number().optional(),\n   293→    qualityThreshold: z.number().optional(),\n   294→    parallelAgents: z.number().optional(),\n   295→    enableStreaming: z.boolean().optional(),\n   296→  }),\n   297→  outputSchema: z.object({\n   298→    success: z.boolean(),\n   299→    researchData: z.any(),\n   300→    report: z.string().optional(),\n   301→    metadata: z.object({\n   302→      qualityScore: z.number(),\n   303→      iterations: z.number(),\n   304→      timestamp: z.string(),\n   305→      insightCount: z.number(),\n   306→      graphSize: z.object({\n   307→        nodes: z.number(),\n   308→        relationships: z.number(),\n   309→      }).optional(),\n   310→    }),\n   311→  }),\n   312→  steps: [initializeStreamingStep, executeStreamingStep, finalizeResultsStep],\n   313→});\n   314→\n   315→// Chain the steps\n   316→streamingResearchWorkflow\n   317→  .then(initializeStreamingStep)\n   318→  .then(executeStreamingStep)\n   319→  .then(finalizeResultsStep)\n   320→  .commit();\n   321→\n   322→// Export streaming utilities for direct use\n   323→export { \n   324→  executeStreamingResearch,\n   325→  executeStreamingResearchSync,\n   326→  ResearchEvent,\n   327→  ResearchEventType,\n   328→} from '../networks/streamingResearchNetwork';\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ]
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "text",
        "text": "Now I have a comprehensive understanding of the streaming research system. Let me update the task file with a detailed Context Manifest:"
      }
    ]
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "tool_use",
        "id": "toolu_01FXp5ufB9J2XtoznH4KishH",
        "name": "Edit",
        "input": {
          "file_path": "/mnt/c/Users/caleb/Desktop/GithubProjects/deepfield/sessions/tasks/h-research-report-depth-quality.md",
          "old_string": "## Context Files\n<!-- Added by context-gathering agent or manually -->\n- @src/mastra/tests/runResearchBaseline.ts\n- @src/mastra/networks/streamingResearchNetwork.ts\n- @src/mastra/agents/researchAgent.ts\n- @src/mastra/agents/synthesisAgent.ts\n- @src/mastra/agents/reportWriterAgent.ts\n- @src/mastra/storage/neo4jStorage.ts",
          "new_string": "## Context Manifest\n\n### How This Currently Works: Streaming Research Report Generation Pipeline\n\nThe system operates as a sophisticated streaming research pipeline that transforms user queries into comprehensive reports through parallel agent coordination, real-time knowledge graph construction, and iterative quality assessment. Here's the complete data flow:\n\n**Entry Point and Orchestration:**\nWhen a user initiates research through `runResearchBaseline.ts` or the streaming network, the query first hits the `executeStreamingResearch` function in `streamingResearchNetwork.ts`. This function serves as the central orchestrator that manages all parallel agents, maintains real-time state, and coordinates the entire research process. The system uses an event-driven architecture with `ResearchEventType` enum defining events like `RESEARCH_STARTED`, `INSIGHT_DISCOVERED`, `GRAPH_UPDATED`, and `RESEARCH_COMPLETE`.\n\n**Phase 1 - Research Plan Creation:**\nThe `researchCoordinatorAgent` receives the query and creates a structured research plan with initial queries, quality criteria, and expected depth requirements. This plan becomes the blueprint for parallel execution. The agent breaks down complex queries into 2-3 focused sub-queries that can be processed simultaneously.\n\n**Phase 2 - Parallel Research Execution:**\nThe system launches multiple research agents in true parallel execution (not blocking). Each `researchAgent` follows a strict two-phase process:\n- Phase 1: Executes 2-3 initial focused searches using `webSearchTool` (powered by Exa API)\n- Phase 2: Processes follow-up questions from Phase 1 learnings, then STOPS (prevents infinite loops)\n\nEach agent uses three critical tools in sequence:\n1. `webSearchTool`: Searches web with livecrawl, returns 2 results per query\n2. `evaluateResultTool`: Pure function that checks relevance using term matching, spam detection, and error identification\n3. `extractLearningsTool`: Extracts key insights and generates targeted follow-up questions\n\n**Phase 3 - Content Processing and Summarization:**\nWhen search results exceed 5000 characters, the `webSummarizationAgent` compresses content while preserving critical insights. This agent reduces content by 80-95% using structured extraction focused on main topics, key insights, supporting details, and context. The summarized content maintains all essential information needed for downstream processing.\n\n**Phase 4 - Learning Extraction and Evaluation:**\nThe `learningExtractionAgent` processes each search result to extract structured learnings with confidence scores and relevance classifications. These learnings are then passed to the `evaluationAgent` which assesses relevance to the original query and filters out unrelated content. Only learnings marked as `keepForProcessing: true` advance to knowledge graph construction.\n\n**Phase 5 - Real-time Knowledge Graph Construction:**\nThe `StreamingKnowledgeGraphAgent` receives insights in real-time and incrementally builds a Neo4j knowledge graph. This agent:\n- Extracts entities (concepts, facts, people, organizations, events, statistics, claims) with confidence scores\n- Maps semantic relationships (causes, supports, contradicts, related_to, leads_to, depends_on)\n- Detects contradictions between sources and tracks resolution attempts\n- Identifies knowledge gaps with importance levels and suggested research queries\n- Uses batch operations (1000 entities per batch) for Neo4j performance optimization\n- Maintains node and relationship caches for deduplication and semantic matching\n\nThe knowledge graph stores all data in Neo4j with proper indexing and supports advanced graph algorithms like PageRank and community detection (if GDS is available).\n\n**Phase 6 - Quality Assessment and Iteration Control:**\nThe `qualityAssessmentAgent` continuously evaluates research completeness using metrics like source count, topic coverage, and depth achievement. It generates structured quality scores (0-100) for completeness, coherence, depth, and overall quality. The system uses a configurable quality threshold (default 75) to determine when research is sufficient.\n\nIf quality is insufficient and iterations remain, the `refinementAgent` generates targeted queries to fill critical gaps identified in the knowledge graph. These refinement queries focus only on high-importance gaps directly related to the original query.\n\n**Phase 7 - Report Generation:**\nThe final report generation happens in the streaming network's completion phase. The `reportAgent` receives:\n- All collected insights with source attribution\n- Knowledge graph metrics (nodes, relationships, coherence scores)\n- Quality assessment results\n- Contradiction and gap analysis\n\nThe report agent synthesizes this information into a comprehensive markdown report with clear sections, but currently lacks proper citations, evidence chains, and confidence levels.\n\n**Current Quality Issues and Data Loss Points:**\n\n1. **Source Attribution Loss**: While insights retain source information during processing, the final report doesn't properly cite sources or maintain evidence chains back to original URLs.\n\n2. **Confidence Score Degradation**: Individual insights have confidence scores, but these aren't propagated through to final report sections or used to qualify statements.\n\n3. **Evidence Chain Breaks**: The system tracks which insights come from which sources, but doesn't maintain the full provenance chain (search query → search result → extracted learning → graph entity → final insight).\n\n4. **Shallow Analysis**: The `reportAgent` instructions emphasize synthesis but don't require deep analytical reasoning or evidence-based justification for conclusions.\n\n5. **Missing Cross-References**: The knowledge graph contains rich relationship data, but reports don't leverage these connections to show how different insights support or contradict each other.\n\n### For New Feature Implementation: Report Depth and Quality Enhancement\n\nSince we're implementing enhanced report depth and quality, several integration points need modification:\n\n**Knowledge Graph Integration Enhancement:**\nThe current streaming knowledge graph contains rich metadata about entity relationships, contradictions, and confidence levels, but this information isn't being effectively utilized in report generation. The `StreamingKnowledgeGraphAgent` already tracks evidence chains and source attribution through its `evidence` field in relationships and `sources` arrays in nodes. We need to modify the report generation to leverage this existing data structure.\n\n**Quality Assessment Integration:**\nThe `qualityAssessmentAgent` already provides detailed metrics including source adequacy, topic coverage, and depth achievement. However, these metrics aren't being used to guide report structure or highlight areas where evidence is weak. The existing `criteriaMetrics` object contains exactly the data needed to add confidence levels to report sections.\n\n**Source Tracking Enhancement:**\nThe current flow maintains source URLs through the entire pipeline: `webSearchTool` returns URLs → `extractLearningsTool` preserves source context → `StreamingKnowledgeGraphAgent` stores in `sources` arrays → but `reportAgent` doesn't utilize this for citations. The infrastructure is present; we need to modify the report generation prompt and output structure.\n\n**Evidence Chain Reconstruction:**\nThe Neo4j storage (`neo4jStorage.ts`) already supports relationship queries and path finding through methods like `findShortestPath`, `findRelatedNodes`, and contradiction detection. These can be used to trace evidence chains from final insights back to source materials.\n\n### Technical Reference Details\n\n#### Component Interfaces & Data Structures\n\n**Knowledge Graph Node Structure:**\n```typescript\ninterface KnowledgeNode {\n  id: string;\n  type: 'concept' | 'fact' | 'person' | 'organization' | 'event' | 'statistic' | 'claim' | 'source';\n  name: string;\n  description: string;\n  confidence: number; // 0-1\n  sources: string[]; // URLs\n  timestamp: string;\n  metadata?: Record<string, any>;\n}\n```\n\n**Knowledge Graph Relationship Structure:**\n```typescript\ninterface KnowledgeRelationship {\n  id: string;\n  from: string; // node id\n  to: string; // node id\n  type: 'causes' | 'contradicts' | 'supports' | 'related_to' | 'part_of' | 'leads_to' | 'depends_on' | 'cites' | 'refutes';\n  strength: number; // 0-1\n  evidence: string; // Supporting evidence text\n  sources: string[]; // Source URLs\n  timestamp: string;\n}\n```\n\n**Research Event Data Structure:**\n```typescript\ninterface ResearchEvent {\n  type: ResearchEventType;\n  timestamp: string;\n  data: any;\n  agentName?: string;\n  confidence?: number;\n}\n```\n\n#### API Interfaces\n\n**Web Search Tool Output:**\n```typescript\n{\n  results: Array<{\n    title: string;\n    url: string;\n    content: string; // Summarized if > 5000 chars\n  }>;\n  error?: string;\n}\n```\n\n**Learning Extraction Output:**\n```typescript\n{\n  learning: string; // Max 500 chars\n  followUpQuestions: string[];\n}\n```\n\n**Quality Assessment Output:**\n```typescript\n{\n  approved: boolean;\n  qualityScore: number; // 0-100\n  criteriaMetrics: {\n    sourceCount: number;\n    sourcesAdequate: boolean;\n    topicsCovered: string[];\n    missingTopics: string[];\n    depthAchieved: 'surface' | 'moderate' | 'deep';\n    depthAdequate: boolean;\n  };\n  gaps: Array<{\n    area: string;\n    description: string;\n    priority: 'high' | 'medium' | 'low';\n  }>;\n  refinementSuggestions?: string[];\n  reasoning: string;\n}\n```\n\n#### Database Operations\n\n**Neo4j Batch Operations:**\n- `neo4jStorage.batchUpsertNodes(nodes: KnowledgeNode[])`: Efficient batch insertion using UNWIND\n- `neo4jStorage.batchUpsertRelationships(relationships: KnowledgeRelationship[])`: Handles node dependencies\n- `neo4jStorage.findShortestPath(fromId: string, toId: string)`: Evidence chain tracing\n- `neo4jStorage.findContradictions()`: Identifies conflicting information\n\n#### Configuration Requirements\n\n**Environment Variables:**\n- `EXA_API_KEY`: For web search functionality\n- `OPENAI_API_KEY`: For all LLM agent operations\n- `NEO4J_URI`: Knowledge graph database connection\n- `NEO4J_USERNAME`: Database authentication\n- `NEO4J_PASSWORD`: Database authentication\n- `NEO4J_DATABASE`: Database name (default: 'neo4j')\n\n**Research Configuration:**\n```typescript\n{\n  maxIterations: 3,\n  qualityThreshold: 75,\n  parallelAgents: 3,\n  progressiveReports: true,\n  timeoutMinutes: 15\n}\n```\n\n#### File Locations for Implementation\n\n**Report Enhancement Implementation:**\n- Primary: `/src/mastra/agents/reportAgent.ts` - Modify instructions and add citation structure\n- Secondary: `/src/mastra/networks/streamingResearchNetwork.ts` - Update report generation section (lines 622-665)\n\n**Quality Tracking Implementation:**\n- `/src/mastra/agents/qualityAssessmentAgent.ts` - Add evidence quality metrics\n- `/src/mastra/agents/streamingKnowledgeGraphAgent.ts` - Enhance evidence tracking\n\n**Citation and Evidence Chain Implementation:**\n- `/src/mastra/storage/neo4jStorage.ts` - Add evidence chain query methods\n- `/src/mastra/tools/extractLearningsTool.ts` - Enhance source attribution\n\n**Testing and Validation:**\n- `/src/mastra/tests/runResearchBaseline.ts` - Add quality metrics and report validation\n\n## Context Files\n<!-- Added by context-gathering agent or manually -->\n- @src/mastra/tests/runResearchBaseline.ts\n- @src/mastra/networks/streamingResearchNetwork.ts\n- @src/mastra/agents/researchAgent.ts\n- @src/mastra/agents/reportAgent.ts\n- @src/mastra/agents/learningExtractionAgent.ts\n- @src/mastra/agents/qualityAssessmentAgent.ts\n- @src/mastra/agents/webSummarizationAgent.ts\n- @src/mastra/agents/knowledgeGraphAgent.ts\n- @src/mastra/agents/streamingKnowledgeGraphAgent.ts\n- @src/mastra/storage/neo4jStorage.ts\n- @src/mastra/tools/webSearchTool.ts\n- @src/mastra/tools/extractLearningsTool.ts\n- @src/mastra/tools/evaluateResultTool.ts\n- @src/mastra/workflows/generateReportWorkflow.ts\n- @src/mastra/workflows/streamingResearchWorkflow.ts"
        }
      }
    ]
  }
]