[
  {
    "role": "user",
    "content": [
      {
        "tool_use_id": "toolu_011UsJNRQpiwGLaosJtg3zof",
        "type": "tool_result",
        "content": "     1→\"\"\"\n     2→Jinja2-based Template Engine for Document Generation\n     3→Supports template inheritance, custom filters, and value propagation\n     4→\"\"\"\n     5→from jinja2 import Environment, FileSystemLoader, Template, select_autoescape\n     6→from typing import Dict, Any, Optional, List\n     7→from pathlib import Path\n     8→import json\n     9→import re\n    10→import sys\n    11→sys.path.append(str(Path(__file__).parent.parent))\n    12→from logger import get_logger\n    13→\n    14→# Set up module logger\n    15→logger = get_logger(\"core.template_engine\")\n    16→\n    17→\n    18→class Jinja2Engine:\n    19→    \"\"\"\n    20→    Template engine using Jinja2 with support for inheritance and custom filters\n    21→    \"\"\"\n    22→    \n    23→    def __init__(self, template_dir: str = \"app/templates\"):\n    24→        self.template_dir = Path(template_dir)\n    25→        self.env = self._setup_environment()\n    26→        self.global_parameters: Dict[str, Any] = {}\n    27→        \n    28→    def _setup_environment(self) -> Environment:\n    29→        \"\"\"Set up Jinja2 environment with custom configuration\"\"\"\n    30→        # Create template directory structure if it doesn't exist\n    31→        if not self.template_dir.exists():\n    32→            self._create_template_structure()\n    33→        \n    34→        # Configure Jinja2 environment\n    35→        env = Environment(\n    36→            loader=FileSystemLoader(str(self.template_dir)),\n    37→            autoescape=select_autoescape(['html', 'xml']),\n    38→            trim_blocks=True,\n    39→            lstrip_blocks=True,\n    40→            keep_trailing_newline=True\n    41→        )\n    42→        \n    43→        # Add custom filters\n    44→        env.filters['capitalize_first'] = self._capitalize_first\n    45→        env.filters['remove_duplicates'] = self._remove_duplicate_phrases\n    46→        env.filters['limit_words'] = self._limit_words\n    47→        env.filters['format_list'] = self._format_list\n    48→        env.filters['ensure_period'] = self._ensure_period\n    49→        \n    50→        # Add custom globals\n    51→        \n    52→        return env\n    53→    \n    54→    def _create_template_structure(self):\n    55→        \"\"\"Create default template directory structure\"\"\"\n    56→        # Create base structure\n    57→        base_dir = self.template_dir / \"base\"\n    58→        base_dir.mkdir(parents=True, exist_ok=True)\n    59→        \n    60→        # Create master template\n    61→        master_template = base_dir / \"master.j2\"\n    62→        if not master_template.exists():\n    63→            master_template.write_text(\"\"\"\n    64→{# Master template for all document types #}\n    65→{% block document_header %}\n    66→{# Document header - can be overridden #}\n    67→{% endblock %}\n    68→\n    69→{% block document_body %}\n    70→{# Main document content #}\n    71→{% endblock %}\n    72→\n    73→{% block document_footer %}\n    74→{# Document footer - can be overridden #}\n    75→{% endblock %}\n    76→\"\"\")\n    77→        \n    78→        # Create validators template\n    79→        validators_template = base_dir / \"validators.j2\"\n    80→        if not validators_template.exists():\n    81→            validators_template.write_text(\"\"\"\n    82→{# Common validation macros #}\n    83→\n    84→{% macro validate_required(value, field_name) %}\n    85→  {% if not value %}\n    86→    {{ raise_error(field_name ~ \" is required\") }}\n    87→  {% endif %}\n    88→{% endmacro %}\n    89→\n    90→{% macro validate_length(value, max_length, field_name) %}\n    91→  {% if value and value|length > max_length %}\n    92→    {{ raise_error(field_name ~ \" exceeds maximum length of \" ~ max_length) }}\n    93→  {% endif %}\n    94→{% endmacro %}\n    95→\n    96→{% macro validate_options(value, allowed_values, field_name) %}\n    97→  {% if value and value not in allowed_values %}\n    98→    {{ raise_error(field_name ~ \" must be one of: \" ~ allowed_values|join(\", \")) }}\n    99→  {% endif %}\n   100→{% endmacro %}\n   101→\"\"\")\n   102→        \n   103→        # Create informed consent directory\n   104→        ic_dir = self.template_dir / \"informed-consent\"\n   105→        ic_dir.mkdir(exist_ok=True)\n   106→        \n   107→        # Create sections directory\n   108→        sections_dir = ic_dir / \"sections\"\n   109→        sections_dir.mkdir(exist_ok=True)\n   110→    \n   111→    def render(self, \n   112→               template_path: str, \n   113→               context: Dict[str, Any],\n   114→               globals: Optional[Dict[str, Any]] = None) -> str:\n   115→        \"\"\"\n   116→        Render a template with the given context\n   117→        \n   118→        Args:\n   119→            template_path: Path to template relative to template_dir\n   120→            context: Template context variables\n   121→            globals: Additional global variables\n   122→            \n   123→        Returns:\n   124→            Rendered template string\n   125→        \"\"\"\n   126→        try:\n   127→            logger.debug(f\"Template engine - trying to load template: {template_path}\")\n   128→            template = self.env.get_template(template_path)\n   129→            logger.debug(f\"Template loaded successfully\")\n   130→            \n   131→            # Merge global parameters\n   132→            full_context = {**self.global_parameters}\n   133→            full_context.update(context)\n   134→            \n   135→            # If generated_content exists, merge it into top-level context for template access\n   136→            if 'generated_content' in context and isinstance(context['generated_content'], dict):\n   137→                full_context.update(context['generated_content'])\n   138→            \n   139→            if globals:\n   140→                full_context.update(globals)\n   141→            \n   142→            logger.debug(f\"Rendering template with context keys: {list(full_context.keys())}\")\n   143→            result = template.render(full_context)\n   144→            logger.debug(f\"Template rendered, result length: {len(result) if result else 0}\")\n   145→            logger.debug(f\"Template engine returning result type: {type(result)}\")\n   146→            logger.debug(f\"First 100 chars of result: {result[:100] if result else 'None'}\")\n   147→            return result\n   148→        except Exception as e:\n   149→            logger.error(f\"Template rendering failed: {e}\", exc_info=True)\n   150→            return None\n   151→    \n   152→    def render_string(self, template_string: str, context: Dict[str, Any]) -> str:\n   153→        \"\"\"Render a template from a string\"\"\"\n   154→        template = self.env.from_string(template_string)\n   155→        full_context = {**self.global_parameters, **context}\n   156→        return template.render(full_context)\n   157→    \n   158→    def set_global_parameters(self, params: Dict[str, Any]):\n   159→        \"\"\"Set global parameters available to all templates\"\"\"\n   160→        self.global_parameters.update(params)\n   161→    \n   162→    \n   163→    # Custom filter functions\n   164→    @staticmethod\n   165→    def _capitalize_first(text: str) -> str:\n   166→        \"\"\"Capitalize only the first letter of the text\"\"\"\n   167→        if not text:\n   168→            return text\n   169→        return text[0].upper() + text[1:]\n   170→    \n   171→    @staticmethod\n   172→    def _remove_duplicate_phrases(text: str) -> str:\n   173→        \"\"\"Remove duplicate phrases from text\"\"\"\n   174→        sentences = text.split('. ')\n   175→        unique_sentences = []\n   176→        for sentence in sentences:\n   177→            if sentence not in unique_sentences:\n   178→                unique_sentences.append(sentence)\n   179→        return '. '.join(unique_sentences)\n   180→    \n   181→    @staticmethod\n   182→    def _limit_words(text: str, word_limit: int) -> str:\n   183→        \"\"\"Limit text to a specific number of words\"\"\"\n   184→        words = text.split()\n   185→        if len(words) <= word_limit:\n   186→            return text\n   187→        return ' '.join(words[:word_limit]) + '...'\n   188→    \n   189→    @staticmethod\n   190→    def _format_list(items: List[str], separator: str = \", \", last_sep: str = \" and \") -> str:\n   191→        \"\"\"Format a list of items with proper separators\"\"\"\n   192→        if not items:\n   193→            return \"\"\n   194→        if len(items) == 1:\n   195→            return items[0]\n   196→        if len(items) == 2:\n   197→            return f\"{items[0]}{last_sep}{items[1]}\"\n   198→        return separator.join(items[:-1]) + last_sep + items[-1]\n   199→    \n   200→    @staticmethod\n   201→    def _ensure_period(text: str) -> str:\n   202→        \"\"\"Ensure text ends with a period\"\"\"\n   203→        if not text:\n   204→            return text\n   205→        text = text.rstrip()\n   206→        if text and text[-1] not in '.!?':\n   207→            return text + '.'\n   208→        return text\n   209→\n   210→    @staticmethod\n   211→    def _fix_articles(text: str) -> str:\n   212→        # Deprecated: rely on LLM to produce natural phrasing\n   213→        return text\n   214→    \n   215→    def create_template_from_slots(self, \n   216→                                   template_text: str, \n   217→                                   slots: List[Any]) -> Template:\n   218→        \"\"\"\n   219→        Create a Jinja2 template from slot-based template text\n   220→        \n   221→        Args:\n   222→            template_text: Template text with {slot_name} placeholders\n   223→            slots: List of TemplateSlot objects\n   224→            \n   225→        Returns:\n   226→            Jinja2 Template object\n   227→        \"\"\"\n   228→        # Convert {slot_name} to {{ slot_name }} for Jinja2\n   229→        jinja_template_text = re.sub(\n   230→            r'\\{(\\w+)\\}',\n   231→            r'{{ \\1 }}',\n   232→            template_text\n   233→        )\n   234→        \n   235→        # Add validation for required slots\n   236→        slot_names = [slot.name for slot in slots if hasattr(slot, 'name')]\n   237→        validation_block = \"{% import 'base/validators.j2' as validators %}\\n\"\n   238→        \n   239→        for slot in slots:\n   240→            if hasattr(slot, 'validation_rules'):\n   241→                rules = slot.validation_rules\n   242→                if rules.get('required'):\n   243→                    validation_block += f\"{{% call validators.validate_required({slot.name}, '{slot.name}') %}}{{% endcall %}}\\n\"\n   244→                if 'max_length' in rules:\n   245→                    validation_block += f\"{{% call validators.validate_length({slot.name}, {rules['max_length']}, '{slot.name}') %}}{{% endcall %}}\\n\"\n   246→        \n   247→        # Combine validation and template\n   248→        full_template = validation_block + jinja_template_text\n   249→        \n   250→        return self.env.from_string(full_template)\n   251→    \n   252→    def list_available_templates(self) -> List[str]:\n   253→        \"\"\"List all available templates\"\"\"\n   254→        templates = []\n   255→        for template_file in self.template_dir.rglob(\"*.j2\"):\n   256→            relative_path = template_file.relative_to(self.template_dir)\n   257→            templates.append(str(relative_path))\n   258→        return sorted(templates)\n   259→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ]
  },
  {
    "role": "user",
    "content": [
      {
        "tool_use_id": "toolu_01QNfPbTVHnkYcRuFPmbH4bw",
        "type": "tool_result",
        "content": "     1→\"\"\"\n     2→Enhanced RAG Pipeline with SPLICE Chunking Method\n     3→Implements Semantic Preservation with Length-Informed Chunking Enhancement\n     4→\"\"\"\n     5→from typing import List, Dict, Any, Optional, Tuple, Generator\n     6→from dataclasses import dataclass, field\n     7→import re\n     8→from collections import deque\n     9→import numpy as np\n    10→from llama_index.core.node_parser import NodeParser, SimpleNodeParser\n    11→from llama_index.core.schema import BaseNode, TextNode, Document\n    12→from llama_index.core import VectorStoreIndex\n    13→from llama_index.core.retrievers import VectorIndexRetriever\n    14→from llama_index.core.query_engine import RetrieverQueryEngine\n    15→from llama_index.core.postprocessor import LLMRerank\n    16→\n    17→\n    18→@dataclass\n    19→class ChunkMetadata:\n    20→    \"\"\"Metadata for a chunk with hierarchical information\"\"\"\n    21→    chunk_id: str\n    22→    parent_id: Optional[str] = None\n    23→    children_ids: List[str] = field(default_factory=list)\n    24→    section_type: Optional[str] = None\n    25→    semantic_group: Optional[int] = None\n    26→    boundary_type: str = \"soft\"  # soft, hard, semantic\n    27→    overlap_tokens: int = 0\n    28→    hierarchical_level: int = 0\n    29→\n    30→\n    31→class SPLICEChunker:\n    32→    \"\"\"\n    33→    SPLICE: Semantic Preservation with Length-Informed Chunking Enhancement\n    34→    Implements structure-aware chunking with semantic boundaries\n    35→    \"\"\"\n    36→    \n    37→    def __init__(self,\n    38→                 min_chunk_size: int = 128,\n    39→                 target_chunk_size: int = 256,\n    40→                 max_chunk_size: int = 512,\n    41→                 overlap_ratio: float = 0.1,\n    42→                 semantic_threshold: float = 0.7):\n    43→        \"\"\"\n    44→        Initialize SPLICE chunker\n    45→        \n    46→        Args:\n    47→            min_chunk_size: Minimum tokens per chunk\n    48→            target_chunk_size: Target tokens per chunk\n    49→            max_chunk_size: Maximum tokens per chunk\n    50→            overlap_ratio: Overlap between chunks as ratio of chunk size\n    51→            semantic_threshold: Threshold for semantic similarity grouping\n    52→        \"\"\"\n    53→        self.min_chunk_size = min_chunk_size\n    54→        self.target_chunk_size = target_chunk_size\n    55→        self.max_chunk_size = max_chunk_size\n    56→        self.overlap_ratio = overlap_ratio\n    57→        self.semantic_threshold = semantic_threshold\n    58→        \n    59→        # Structural markers for different document types\n    60→        self.structure_markers = {\n    61→            'section': r'^(?:Section|SECTION|Part|PART)\\s+\\d+',\n    62→            'subsection': r'^(?:\\d+\\.\\d+|\\([a-z]\\)|\\w\\))',\n    63→            'list_item': r'^(?:[-•*]|\\d+[.)]\\s)',\n    64→            'paragraph_break': r'\\n\\n+',\n    65→            'sentence_end': r'[.!?]\\s+',\n    66→            'header': r'^(?:[A-Z][A-Z\\s]{2,}:?|#{1,6}\\s+)',\n    67→        }\n    68→    \n    69→    def identify_boundaries(self, text: str) -> List[Tuple[int, str]]:\n    70→        \"\"\"\n    71→        Identify structural boundaries in text\n    72→        \n    73→        Returns:\n    74→            List of (position, boundary_type) tuples\n    75→        \"\"\"\n    76→        boundaries = []\n    77→        \n    78→        for boundary_type, pattern in self.structure_markers.items():\n    79→            for match in re.finditer(pattern, text, re.MULTILINE):\n    80→                boundaries.append((match.start(), boundary_type))\n    81→        \n    82→        # Sort by position\n    83→        boundaries.sort(key=lambda x: x[0])\n    84→        return boundaries\n    85→    \n    86→    def calculate_semantic_groups(self, chunks: List[str]) -> List[int]:\n    87→        \"\"\"\n    88→        Group chunks by semantic similarity\n    89→        Simple implementation - in production would use embeddings\n    90→        \"\"\"\n    91→        # For now, use simple keyword overlap as proxy for semantic similarity\n    92→        groups = []\n    93→        current_group = 0\n    94→        \n    95→        for i, chunk in enumerate(chunks):\n    96→            if i == 0:\n    97→                groups.append(current_group)\n    98→                continue\n    99→            \n   100→            # Calculate simple similarity with previous chunk\n   101→            prev_words = set(chunks[i-1].lower().split())\n   102→            curr_words = set(chunk.lower().split())\n   103→            \n   104→            if not prev_words:\n   105→                similarity = 0\n   106→            else:\n   107→                similarity = len(prev_words & curr_words) / len(prev_words | curr_words)\n   108→            \n   109→            if similarity < self.semantic_threshold:\n   110→                current_group += 1\n   111→            \n   112→            groups.append(current_group)\n   113→        \n   114→        return groups\n   115→    \n   116→    def apply_overlap_policy(self, chunks: List[str], boundaries: List[Tuple[int, str]]) -> List[str]:\n   117→        \"\"\"\n   118→        Apply intelligent overlap based on boundary types and semantic similarity\n   119→        \"\"\"\n   120→        enhanced_chunks = []\n   121→        \n   122→        for i, chunk in enumerate(chunks):\n   123→            if i == 0:\n   124→                enhanced_chunks.append(chunk)\n   125→                continue\n   126→            \n   127→            # Determine overlap size based on boundary type\n   128→            overlap_tokens = int(len(chunk.split()) * self.overlap_ratio)\n   129→            \n   130→            # Get tokens from previous chunk for overlap\n   131→            prev_tokens = chunks[i-1].split()\n   132→            \n   133→            if overlap_tokens > 0 and len(prev_tokens) >= overlap_tokens:\n   134→                overlap_text = ' '.join(prev_tokens[-overlap_tokens:])\n   135→                enhanced_chunk = overlap_text + ' ' + chunk\n   136→            else:\n   137→                enhanced_chunk = chunk\n   138→            \n   139→            enhanced_chunks.append(enhanced_chunk)\n   140→        \n   141→        return enhanced_chunks\n   142→    \n   143→    def chunk_document(self, text: str) -> List[Tuple[str, ChunkMetadata]]:\n   144→        \"\"\"\n   145→        Chunk document using SPLICE method\n   146→        \n   147→        Returns:\n   148→            List of (chunk_text, metadata) tuples\n   149→        \"\"\"\n   150→        # Identify structural boundaries\n   151→        boundaries = self.identify_boundaries(text)\n   152→        \n   153→        # Create initial chunks respecting boundaries\n   154→        chunks = []\n   155→        current_chunk = []\n   156→        current_tokens = 0\n   157→        current_pos = 0\n   158→        chunk_boundaries = []\n   159→        \n   160→        tokens = text.split()\n   161→        \n   162→        for token_idx, token in enumerate(tokens):\n   163→            current_chunk.append(token)\n   164→            current_tokens += 1\n   165→            \n   166→            # Check if we're at a boundary\n   167→            token_pos = len(' '.join(tokens[:token_idx+1]))\n   168→            boundary_type = None\n   169→            for bound_pos, bound_type in boundaries:\n   170→                if abs(token_pos - bound_pos) < 10:  # Within 10 chars of boundary\n   171→                    boundary_type = bound_type\n   172→                    break\n   173→            \n   174→            # Decide whether to create a chunk\n   175→            should_chunk = False\n   176→            \n   177→            if boundary_type in ['section', 'subsection', 'header']:\n   178→                # Hard boundary - always chunk unless too small\n   179→                if current_tokens >= self.min_chunk_size:\n   180→                    should_chunk = True\n   181→            elif current_tokens >= self.target_chunk_size:\n   182→                # Reached target size - look for soft boundary\n   183→                if boundary_type in ['paragraph_break', 'sentence_end']:\n   184→                    should_chunk = True\n   185→                elif current_tokens >= self.max_chunk_size:\n   186→                    # Force chunk at max size\n   187→                    should_chunk = True\n   188→            \n   189→            if should_chunk:\n   190→                chunk_text = ' '.join(current_chunk)\n   191→                chunks.append(chunk_text)\n   192→                chunk_boundaries.append(boundary_type or 'soft')\n   193→                current_chunk = []\n   194→                current_tokens = 0\n   195→        \n   196→        # Add remaining text as final chunk\n   197→        if current_chunk:\n   198→            chunks.append(' '.join(current_chunk))\n   199→            chunk_boundaries.append('soft')\n   200→        \n   201→        # Calculate semantic groups\n   202→        semantic_groups = self.calculate_semantic_groups(chunks)\n   203→        \n   204→        # Apply overlap policy\n   205→        enhanced_chunks = self.apply_overlap_policy(chunks, boundaries)\n   206→        \n   207→        # Create metadata for each chunk\n   208→        result = []\n   209→        for i, chunk in enumerate(enhanced_chunks):\n   210→            metadata = ChunkMetadata(\n   211→                chunk_id=f\"chunk_{i}\",\n   212→                parent_id=f\"chunk_{i-1}\" if i > 0 else None,\n   213→                children_ids=[f\"chunk_{i+1}\"] if i < len(enhanced_chunks) - 1 else [],\n   214→                semantic_group=semantic_groups[i] if i < len(semantic_groups) else 0,\n   215→                boundary_type=chunk_boundaries[i] if i < len(chunk_boundaries) else 'soft',\n   216→                overlap_tokens=int(len(chunk.split()) * self.overlap_ratio) if i > 0 else 0,\n   217→                hierarchical_level=0  # Would be set based on document structure\n   218→            )\n   219→            result.append((chunk, metadata))\n   220→        \n   221→        return result\n   222→\n   223→\n   224→class StreamingRAGPipeline:\n   225→    \"\"\"\n   226→    Enhanced RAG pipeline with streaming capabilities and SPLICE chunking\n   227→    \"\"\"\n   228→    \n   229→    def __init__(self, \n   230→                 chunking_method: str = \"SPLICE\",\n   231→                 embed_model: Any = None,\n   232→                 llm: Any = None):\n   233→        \"\"\"\n   234→        Initialize streaming RAG pipeline\n   235→        \n   236→        Args:\n   237→            chunking_method: Chunking method to use (SPLICE or traditional)\n   238→            embed_model: Embedding model for vector store\n   239→            llm: Language model for generation\n   240→        \"\"\"\n   241→        self.chunking_method = chunking_method\n   242→        self.embed_model = embed_model\n   243→        self.llm = llm\n   244→        \n   245→        if chunking_method == \"SPLICE\":\n   246→            self.chunker = SPLICEChunker()\n   247→        else:\n   248→            self.chunker = SimpleNodeParser(\n   249→                chunk_size=256,\n   250→                chunk_overlap=20\n   251→            )\n   252→        \n   253→        self.vector_store_index: Optional[VectorStoreIndex] = None\n   254→        self.query_engine: Optional[RetrieverQueryEngine] = None\n   255→        \n   256→        self.chunk_metadata_store: Dict[str, ChunkMetadata] = {}\n   257→    \n   258→    def process_document(self, document: Document) -> List[BaseNode]:\n   259→        \"\"\"\n   260→        Process document into chunks using selected method\n   261→        \"\"\"\n   262→        if self.chunking_method == \"SPLICE\":\n   263→            # Use SPLICE chunking\n   264→            chunks_with_metadata = self.chunker.chunk_document(document.text)\n   265→            \n   266→            nodes = []\n   267→            for chunk_text, metadata in chunks_with_metadata:\n   268→                node = TextNode(\n   269→                    text=chunk_text,\n   270→                    metadata={\n   271→                        \"chunk_id\": metadata.chunk_id,\n   272→                        \"parent_id\": metadata.parent_id,\n   273→                        \"semantic_group\": metadata.semantic_group,\n   274→                        \"boundary_type\": metadata.boundary_type,\n   275→                        \"hierarchical_level\": metadata.hierarchical_level\n   276→                    }\n   277→                )\n   278→                nodes.append(node)\n   279→                # Store metadata for later use\n   280→                self.chunk_metadata_store[metadata.chunk_id] = metadata\n   281→            \n   282→            return nodes\n   283→        else:\n   284→            # Use traditional chunking\n   285→            return self.chunker.get_nodes_from_documents([document])\n   286→    \n   287→    def build_index(self, nodes: List[BaseNode]):\n   288→        \"\"\"Build vector store index from nodes\"\"\"\n   289→        self.vector_store_index = VectorStoreIndex(\n   290→            nodes,\n   291→            embed_model=self.embed_model\n   292→        )\n   293→        \n   294→        # Set up retriever with reranking\n   295→        retriever = VectorIndexRetriever(\n   296→            index=self.vector_store_index,\n   297→            similarity_top_k=8\n   298→        )\n   299→        \n   300→        # Configure query engine with LLM reranking\n   301→        rerank_postprocessor = LLMRerank(\n   302→            llm=self.llm,\n   303→            top_n=4\n   304→        )\n   305→        \n   306→        self.query_engine = RetrieverQueryEngine(\n   307→            retriever=retriever,\n   308→            node_postprocessors=[rerank_postprocessor]\n   309→        )\n   310→    \n   311→    def stream_query(self, query: str) -> Generator[str, None, None]:\n   312→        \"\"\"\n   313→        Stream query results progressively\n   314→        \n   315→        Yields:\n   316→            Chunks of generated text as they become available\n   317→        \"\"\"\n   318→        if not self.query_engine:\n   319→            yield \"Error: Index not built. Process documents first.\"\n   320→            return\n   321→        \n   322→        # Get initial retrieval results\n   323→        response = self.query_engine.query(query)\n   324→        \n   325→        # Stream the response in chunks\n   326→        text = str(response)\n   327→        words = text.split()\n   328→        \n   329→        # Stream words in small batches for progressive rendering\n   330→        batch_size = 5\n   331→        for i in range(0, len(words), batch_size):\n   332→            batch = words[i:i + batch_size]\n   333→            yield ' '.join(batch) + ' '\n   334→    \n   335→    def get_hierarchical_context(self, chunk_id: str) -> Dict[str, Any]:\n   336→        \"\"\"\n   337→        Get hierarchical context for a chunk including parents and siblings\n   338→        \"\"\"\n   339→        if chunk_id not in self.chunk_metadata_store:\n   340→            return {}\n   341→        \n   342→        metadata = self.chunk_metadata_store[chunk_id]\n   343→        context = {\n   344→            \"current\": chunk_id,\n   345→            \"parent\": metadata.parent_id,\n   346→            \"children\": metadata.children_ids,\n   347→            \"semantic_group\": metadata.semantic_group,\n   348→            \"siblings\": []\n   349→        }\n   350→        \n   351→        # Find siblings (same semantic group)\n   352→        for other_id, other_metadata in self.chunk_metadata_store.items():\n   353→            if (other_id != chunk_id and \n   354→                other_metadata.semantic_group == metadata.semantic_group):\n   355→                context[\"siblings\"].append(other_id)\n   356→        \n   357→        return context\n   358→    \n   359→    def update_chunk(self, chunk_id: str, new_text: str):\n   360→        \"\"\"\n   361→        Update a chunk in real-time (for streaming updates)\n   362→        \"\"\"\n   363→        # Update completed\n   364→        \n   365→        # Mark index for rebuild\n   366→        # In production, would use incremental index updates\n   367→        if self.vector_store_index:\n   368→            # Simple approach: rebuild affected parts of index\n   369→            # Advanced: Use incremental vector updates\n   370→            pass\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ]
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "text",
        "text": "Now let me examine the remaining core modules and the plugin implementations:"
      }
    ]
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "tool_use",
        "id": "toolu_01Dgyqqkx2BP8tGSbDA4GG13",
        "name": "Read",
        "input": {
          "file_path": "/mnt/d/Common_Resources/irb-ki-summary/app/core/llm_integration.py"
        }
      }
    ]
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "tool_use",
        "id": "toolu_01R2BdsNPE7tdwMiszGmuXGc",
        "name": "Read",
        "input": {
          "file_path": "/mnt/d/Common_Resources/irb-ki-summary/app/core/extraction_models.py"
        }
      }
    ]
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "tool_use",
        "id": "toolu_01QzPduwJzLCnWdu5DxtaxHt",
        "name": "Read",
        "input": {
          "file_path": "/mnt/d/Common_Resources/irb-ki-summary/app/plugins/informed_consent_plugin.py"
        }
      }
    ]
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "tool_use",
        "id": "toolu_011RpiKLLJxqDMG64jLWJb9Q",
        "name": "Read",
        "input": {
          "file_path": "/mnt/d/Common_Resources/irb-ki-summary/app/plugins/clinical_protocol_plugin.py"
        }
      }
    ]
  },
  {
    "role": "user",
    "content": [
      {
        "tool_use_id": "toolu_01Dgyqqkx2BP8tGSbDA4GG13",
        "type": "tool_result",
        "content": "     1→\"\"\"\n     2→Generic Azure OpenAI Integration with Structured Outputs\n     3→Provides base LLM functionality with Pydantic model support\n     4→\"\"\"\n     5→import os\n     6→import json\n     7→from typing import Dict, Any, List, Optional, Union, Type\n     8→from dotenv import load_dotenv\n     9→from pathlib import Path\n    10→from pydantic import BaseModel\n    11→from llama_index.llms.azure_openai import AzureOpenAI as AzureOpenAILLM\n    12→from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n    13→from llama_index.core.llms import ChatMessage\n    14→from llama_index.core.prompts import PromptTemplate\n    15→\n    16→# Load environment variables from app/.env\n    17→env_path = Path(__file__).parent.parent / '.env'\n    18→load_dotenv(env_path)\n    19→\n    20→\n    21→class GenericLLMExtractor:\n    22→    \"\"\"\n    23→    Generic Azure OpenAI client for document extraction\n    24→    \"\"\"\n    25→    \n    26→    def __init__(self):\n    27→        \"\"\"Initialize Azure OpenAI client using LlamaIndex with structured output support\"\"\"\n    28→        organization = os.getenv(\"ORGANIZATION\", \"231173\")\n    29→        api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n    30→        azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n    31→        # Use latest API version for structured outputs\n    32→        api_version = os.getenv(\"API_VERSION\", \"2024-10-21\")\n    33→        \n    34→        # Use LlamaIndex Azure OpenAI client with structured output support\n    35→        self.llm = AzureOpenAILLM(\n    36→            model=\"gpt-4o\",\n    37→            engine=\"gpt-4o\",\n    38→            deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_LLM\", \"text-embedding-3-small\"),\n    39→            api_key=api_key,\n    40→            azure_endpoint=azure_endpoint,\n    41→            api_version=api_version,\n    42→            default_headers={\"OpenAI-Organization\": organization, \"Shortcode\": organization},\n    43→            temperature=0,  # Minimum temperature for consistency\n    44→            top_p=0.0,      # Deterministic sampling\n    45→            organization=organization,\n    46→        )\n    47→        \n    48→        self.deployment = os.getenv(\"DEPLOYMENT_ID\", \"gpt-4o\")\n    49→    \n    50→    async def extract_structured(self,\n    51→                                document_context: str,\n    52→                                output_cls: Type[BaseModel],\n    53→                                system_prompt: str = None) -> BaseModel:\n    54→        \"\"\"\n    55→        Extract structured data using Pydantic models with guaranteed schema compliance\n    56→        \n    57→        Args:\n    58→            document_context: Document text to extract from\n    59→            output_cls: Pydantic model class defining the output structure\n    60→            system_prompt: Optional custom system prompt\n    61→            \n    62→        Returns:\n    63→            Instance of the Pydantic model with extracted data\n    64→        \"\"\"\n    65→        try:\n    66→            # Build system prompt\n    67→            if not system_prompt:\n    68→                system_prompt = (\n    69→                    \"You are an expert at extracting structured information from documents. \"\n    70→                    \"Extract the requested information accurately and concisely. \"\n    71→                    \"Follow the schema requirements strictly.\"\n    72→                )\n    73→            \n    74→            # Convert LLM to structured LLM with Pydantic model\n    75→            structured_llm = self.llm.as_structured_llm(output_cls=output_cls)\n    76→            \n    77→            # Build messages\n    78→            messages = [\n    79→                ChatMessage(role=\"system\", content=system_prompt),\n    80→                ChatMessage(role=\"user\", content=f\"Extract information from this document:\\n\\n{document_context[:8000]}\")\n    81→            ]\n    82→            \n    83→            # Get structured response\n    84→            response = structured_llm.chat(messages)\n    85→            \n    86→            # The response.raw contains the Pydantic model instance\n    87→            result = response.raw\n    88→            \n    89→            return result\n    90→            \n    91→        except Exception as e:\n    92→            print(f\"Structured extraction error: {e}\")\n    93→            # Return default instance on error\n    94→            return output_cls()\n    95→    \n    96→    async def extract_json(self, \n    97→                          document_context: str,\n    98→                          extraction_schema: Dict[str, Any],\n    99→                          system_prompt: str = None) -> Dict[str, Any]:\n   100→        \"\"\"\n   101→        Legacy JSON extraction - now uses structured extraction internally\n   102→        \n   103→        Args:\n   104→            document_context: Document text to extract from\n   105→            extraction_schema: Dictionary describing what to extract\n   106→            system_prompt: Optional custom system prompt\n   107→            \n   108→        Returns:\n   109→            Dictionary with extracted values\n   110→        \"\"\"\n   111→        # Check if we have a Pydantic model available\n   112→        from app.core.extraction_models import KIExtractionSchema\n   113→        \n   114→        # For KI extraction, use the Pydantic model\n   115→        if \"is_pediatric\" in extraction_schema and \"study_type\" in extraction_schema:\n   116→            try:\n   117→                result = await self.extract_structured(\n   118→                    document_context=document_context,\n   119→                    output_cls=KIExtractionSchema,\n   120→                    system_prompt=system_prompt\n   121→                )\n   122→                # Convert Pydantic model to dict\n   123→                return result.model_dump()\n   124→            except Exception as e:\n   125→                print(f\"Structured extraction failed, falling back: {e}\")\n   126→        \n   127→        # Fallback to old method for other schemas\n   128→        try:\n   129→            # Build system prompt\n   130→            if not system_prompt:\n   131→                system_prompt = (\n   132→                    \"You are an expert at extracting structured information from documents. \"\n   133→                    \"Extract the requested information accurately and concisely. \"\n   134→                    \"Return a valid JSON object with all requested fields.\"\n   135→                )\n   136→            \n   137→            # Build user prompt with schema\n   138→            user_prompt = (\n   139→                f\"Extract the following information from this document:\\n\\n\"\n   140→                f\"Required fields:\\n{json.dumps(extraction_schema, indent=2)}\\n\\n\"\n   141→                f\"Document:\\n{document_context[:8000]}\"\n   142→            )\n   143→            \n   144→            messages = [\n   145→                ChatMessage(role=\"system\", content=system_prompt),\n   146→                ChatMessage(role=\"user\", content=user_prompt)\n   147→            ]\n   148→            \n   149→            # Use LlamaIndex LLM for chat completion\n   150→            response = self.llm.chat(messages)\n   151→            \n   152→            # Parse response\n   153→            response_text = response.message.content\n   154→            # Try to extract JSON from the response\n   155→            try:\n   156→                result = json.loads(response_text)\n   157→            except json.JSONDecodeError:\n   158→                # Try to find JSON in the response\n   159→                import re\n   160→                json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n   161→                if json_match:\n   162→                    result = json.loads(json_match.group())\n   163→                else:\n   164→                    print(f\"Could not parse JSON from response: {response_text[:200]}\")\n   165→                    return {\"error\": \"Failed to parse JSON\"}\n   166→            \n   167→            return result\n   168→            \n   169→        except Exception as e:\n   170→            print(f\"Extraction error: {e}\")\n   171→            return {\"error\": str(e)}\n   172→    \n   173→    async def extract_text(self, \n   174→                          document_context: str,\n   175→                          query: str,\n   176→                          max_words: int = 50) -> str:\n   177→        \"\"\"\n   178→        Extract specific text based on a query\n   179→        \n   180→        Args:\n   181→            document_context: Document to extract from\n   182→            query: What to extract\n   183→            max_words: Maximum words in response\n   184→            \n   185→        Returns:\n   186→            Extracted text string\n   187→        \"\"\"\n   188→        try:\n   189→            messages = [\n   190→                ChatMessage(\n   191→                    role=\"system\",\n   192→                    content=f\"Extract the requested information in {max_words} words or less.\"\n   193→                ),\n   194→                ChatMessage(\n   195→                    role=\"user\",\n   196→                    content=f\"{query}\\n\\nDocument:\\n{document_context[:4000]}\"\n   197→                )\n   198→            ]\n   199→            \n   200→            # Use LlamaIndex LLM for chat completion\n   201→            response = self.llm.chat(messages)\n   202→            \n   203→            result = response.message.content.strip()\n   204→            return result\n   205→            \n   206→        except Exception as e:\n   207→            print(f\"Text extraction error: {e}\")\n   208→            return \"\"\n   209→    \n   210→    async def extract_boolean(self,\n   211→                            document_context: str,\n   212→                            query: str) -> bool:\n   213→        \"\"\"\n   214→        Extract a yes/no answer based on a query\n   215→        \n   216→        Args:\n   217→            document_context: Document to analyze\n   218→            query: Yes/no question about the document\n   219→            \n   220→        Returns:\n   221→            Boolean result\n   222→        \"\"\"\n   223→        try:\n   224→            messages = [\n   225→                {\n   226→                    \"role\": \"system\",\n   227→                    \"content\": \"Answer the question with only 'YES' or 'NO' based on the document.\"\n   228→                },\n   229→                {\n   230→                    \"role\": \"user\",\n   231→                    \"content\": f\"{query}\\n\\nDocument:\\n{document_context[:4000]}\"\n   232→                }\n   233→            ]\n   234→            \n   235→            # Use LlamaIndex LLM for chat completion\n   236→            response = self.llm.chat(messages)\n   237→            \n   238→            answer = response.message.content.strip().upper()\n   239→            result = answer == \"YES\"\n   240→            return result\n   241→            \n   242→        except Exception as e:\n   243→            print(f\"Boolean extraction error: {e}\")\n   244→            return False\n   245→    \n   246→    async def extract_enum(self,\n   247→                         document_context: str,\n   248→                         query: str,\n   249→                         allowed_values: List[str]) -> str:\n   250→        \"\"\"\n   251→        Extract a value from a list of allowed options\n   252→        \n   253→        Args:\n   254→            document_context: Document to analyze\n   255→            query: What to determine\n   256→            allowed_values: List of valid options\n   257→            \n   258→        Returns:\n   259→            One of the allowed values\n   260→        \"\"\"\n   261→        try:\n   262→            messages = [\n   263→                ChatMessage(\n   264→                    role=\"system\",\n   265→                    content=(\n   266→                        f\"Based on the document, select the most appropriate option from: \"\n   267→                        f\"{', '.join(allowed_values)}. Return only the selected option.\"\n   268→                    )\n   269→                ),\n   270→                ChatMessage(\n   271→                    role=\"user\",\n   272→                    content=f\"{query}\\n\\nDocument:\\n{document_context[:4000]}\"\n   273→                )\n   274→            ]\n   275→            \n   276→            # Use LlamaIndex LLM for chat completion\n   277→            response = self.llm.chat(messages)\n   278→            \n   279→            result = response.message.content.strip()\n   280→            # Validate it's in allowed values\n   281→            if result not in allowed_values and allowed_values:\n   282→                result = allowed_values[0]  # Default to first option\n   283→            \n   284→            return result\n   285→            \n   286→        except Exception as e:\n   287→            print(f\"Enum extraction error: {e}\")\n   288→            return allowed_values[0] if allowed_values else \"\"\n   289→    \n   290→    async def complete(self,\n   291→                      prompt: str,\n   292→                      system_prompt: str = None,\n   293→                      max_tokens: int = 500,\n   294→                      temperature: float = 0.7) -> str:\n   295→        \"\"\"\n   296→        Generic completion for any prompt\n   297→        \n   298→        Args:\n   299→            prompt: User prompt\n   300→            system_prompt: Optional system prompt\n   301→            max_tokens: Maximum response tokens\n   302→            temperature: Sampling temperature\n   303→            \n   304→        Returns:\n   305→            Completion text\n   306→        \"\"\"\n   307→        try:\n   308→            messages = []\n   309→            if system_prompt:\n   310→                messages.append(ChatMessage(role=\"system\", content=system_prompt))\n   311→            messages.append(ChatMessage(role=\"user\", content=prompt))\n   312→            \n   313→            # Use LlamaIndex LLM for chat completion\n   314→            response = self.llm.chat(messages)\n   315→            \n   316→            return response.message.content\n   317→            \n   318→        except Exception as e:\n   319→            print(f\"Completion error: {e}\")\n   320→            return \"\"\n   321→    \n   322→\n   323→\n   324→# Global instance\n   325→_extractor_instance = None\n   326→\n   327→def get_generic_extractor() -> GenericLLMExtractor:\n   328→    \"\"\"Get or create global generic extractor instance\"\"\"\n   329→    global _extractor_instance\n   330→    if _extractor_instance is None:\n   331→        _extractor_instance = GenericLLMExtractor()\n   332→    return _extractor_instance\n   333→\n   334→# Backward compatibility alias\n   335→def get_extractor():\n   336→    \"\"\"Backward compatibility - returns generic extractor\"\"\"\n   337→    return get_generic_extractor()\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ]
  },
  {
    "role": "user",
    "content": [
      {
        "tool_use_id": "toolu_01R2BdsNPE7tdwMiszGmuXGc",
        "type": "tool_result",
        "content": "     1→\"\"\"\n     2→Pydantic models for structured extraction\n     3→Defines strongly-typed models for extracting information from documents\n     4→\"\"\"\n     5→from pydantic import BaseModel, Field\n     6→from typing import Optional, List, Literal\n     7→from enum import Enum\n     8→\n     9→\n    10→class StudyType(str, Enum):\n    11→    \"\"\"Types of studies\"\"\"\n    12→    STUDYING = \"studying\"\n    13→    COLLECTING = \"collecting\"\n    14→\n    15→\n    16→class ArticleType(str, Enum):\n    17→    \"\"\"Article types for study objects\"\"\"\n    18→    A = \"a \"\n    19→    A_NEW = \"a new \"\n    20→    NONE = \"\"\n    21→\n    22→\n    23→class PopulationType(str, Enum):\n    24→    \"\"\"Population types for studies\"\"\"\n    25→    PEOPLE = \"people\"\n    26→    LARGE_NUMBERS_PEOPLE = \"large numbers of people\"\n    27→    SMALL_NUMBERS_PEOPLE = \"small numbers of people\"\n    28→    CHILDREN = \"children\"\n    29→    LARGE_NUMBERS_CHILDREN = \"large numbers of children\"\n    30→    SMALL_NUMBERS_CHILDREN = \"small numbers of children\"\n    31→\n    32→\n    33→class KIExtractionSchema(BaseModel):\n    34→    \"\"\"\n    35→    Schema for extracting Key Information from Informed Consent documents\n    36→    All fields have strict validation and word limits\n    37→    \"\"\"\n    38→    # Section 1 - Eligibility\n    39→    is_pediatric: bool = Field(\n    40→        default=False,\n    41→        description=\"Are children eligible to participate? Look for age requirements, pediatric participants, or parent/guardian consent.\"\n    42→    )\n    43→    \n    44→    # Section 4 - Study Description  \n    45→    study_type: StudyType = Field(\n    46→        default=StudyType.STUDYING,\n    47→        description=\"Is this study primarily studying/testing something or collecting/gathering something?\"\n    48→    )\n    49→    \n    50→    article: ArticleType = Field(\n    51→        default=ArticleType.A,\n    52→        description=\"What article should precede the study object? 'a ' for existing, 'a new ' for novel, '' for plural/uncountable\"\n    53→    )\n    54→    \n    55→    study_object: str = Field(\n    56→        default=\"intervention\",\n    57→        max_length=150,\n    58→        description=\"Main object being studied (e.g., drug, device, procedure). Include FDA status if mentioned. Lowercase. Max 30 words.\"\n    59→    )\n    60→    \n    61→    population: PopulationType = Field(\n    62→        default=PopulationType.PEOPLE,\n    63→        description=\"What population will participate?\"\n    64→    )\n    65→    \n    66→    study_purpose: str = Field(\n    67→        default=\"to evaluate effectiveness\",\n    68→        max_length=100,\n    69→        description=\"Main purpose of study in 10-15 words. Simple language, no intro phrases.\"\n    70→    )\n    71→    \n    72→    study_goals: str = Field(\n    73→        default=\"to gather data\",\n    74→        max_length=100,\n    75→        description=\"What study will accomplish in 10-15 words. Simple language, direct statement.\"\n    76→    )\n    77→    \n    78→    # Section 5 - Randomization and Washout\n    79→    has_randomization: bool = Field(\n    80→        default=False,\n    81→        description=\"Does document contain words 'randomize', 'randomization', or 'randomized'?\"\n    82→    )\n    83→    \n    84→    requires_washout: bool = Field(\n    85→        default=False,\n    86→        description=\"Does study require stopping medications before/during participation?\"\n    87→    )\n    88→    \n    89→    # Section 6 - Risks\n    90→    key_risks: str = Field(\n    91→        default=\"standard medical risks\",\n    92→        max_length=150,\n    93→        description=\"2-3 most important risks from study (not standard care). Focus on pain/distress. 30 words max.\"\n    94→    )\n    95→    \n    96→    # Section 7 - Benefits\n    97→    has_direct_benefits: bool = Field(\n    98→        default=False,\n    99→        description=\"Are there meaningful direct personal benefits to participants?\"\n   100→    )\n   101→    \n   102→    benefit_description: str = Field(\n   103→        default=\"helping advance medical knowledge\",\n   104→        max_length=100,\n   105→        description=\"Benefits summary to complete 'by [text]'. Don't include 'by'. 20 words max.\"\n   106→    )\n   107→    \n   108→    # Section 8 - Duration\n   109→    study_duration: str = Field(\n   110→        default=\"the study period\",\n   111→        max_length=50,\n   112→        description=\"How long participants will be in study (e.g., '6 months', 'up to 2 years'). 10 words max.\"\n   113→    )\n   114→    \n   115→    # Section 9 - Alternatives\n   116→    affects_treatment: bool = Field(\n   117→        default=False,\n   118→        description=\"Does participation affect current/future treatment options?\"\n   119→    )\n   120→    \n   121→    alternative_options: Optional[str] = Field(\n   122→        default=None,\n   123→        max_length=100,\n   124→        description=\"Treatment alternatives if study affects options. 20 words max.\"\n   125→    )\n   126→    \n   127→    # Additional fields\n   128→    collects_biospecimens: bool = Field(\n   129→        default=False,\n   130→        description=\"Will biological specimens (blood, tissue, DNA, etc.) be collected?\"\n   131→    )\n   132→    \n   133→    biospecimen_details: Optional[str] = Field(\n   134→        default=None,\n   135→        max_length=100,\n   136→        description=\"Brief statement about what specimens will be collected. 20 words max.\"\n   137→    )\n   138→\n   139→\n   140→class ClinicalProtocolExtractionSchema(BaseModel):\n   141→    \"\"\"\n   142→    Schema for extracting information from Clinical Protocol documents\n   143→    \"\"\"\n   144→    protocol_title: str = Field(\n   145→        description=\"Full title of the clinical protocol\"\n   146→    )\n   147→    \n   148→    protocol_number: str = Field(\n   149→        description=\"Protocol number or identifier\"\n   150→    )\n   151→    \n   152→    sponsor: str = Field(\n   153→        description=\"Study sponsor organization\"\n   154→    )\n   155→    \n   156→    phase: Optional[str] = Field(\n   157→        default=None,\n   158→        description=\"Clinical trial phase (e.g., Phase 1, Phase 2, Phase 3)\"\n   159→    )\n   160→    \n   161→    primary_endpoint: str = Field(\n   162→        description=\"Primary study endpoint\"\n   163→    )\n   164→    \n   165→    secondary_endpoints: List[str] = Field(\n   166→        default_factory=list,\n   167→        description=\"List of secondary endpoints\"\n   168→    )\n   169→    \n   170→    inclusion_criteria: List[str] = Field(\n   171→        default_factory=list,\n   172→        description=\"Key inclusion criteria\"\n   173→    )\n   174→    \n   175→    exclusion_criteria: List[str] = Field(\n   176→        default_factory=list,\n   177→        description=\"Key exclusion criteria\"\n   178→    )\n   179→    \n   180→    study_design: str = Field(\n   181→        description=\"Study design (e.g., randomized, double-blind, placebo-controlled)\"\n   182→    )\n   183→    \n   184→    sample_size: Optional[int] = Field(\n   185→        default=None,\n   186→        description=\"Target sample size\"\n   187→    )\n   188→    \n   189→    study_duration: str = Field(\n   190→        description=\"Total study duration\"\n   191→    )\n   192→    \n   193→    regulatory_section: Literal[\"device\", \"drug\", \"biologic\"] = Field(\n   194→        description=\"Type of regulatory section\"\n   195→    )\n   196→    \n   197→    therapeutic_area: str = Field(\n   198→        description=\"Therapeutic area (e.g., cardiovascular, oncology)\"\n   199→    )\n   200→\n   201→\n   202→class GenericExtractionSchema(BaseModel):\n   203→    \"\"\"\n   204→    Generic schema for document extraction when specific schema is not available\n   205→    \"\"\"\n   206→    title: Optional[str] = Field(\n   207→        default=None,\n   208→        description=\"Document title if present\"\n   209→    )\n   210→    \n   211→    summary: str = Field(\n   212→        description=\"Brief summary of the document\"\n   213→    )\n   214→    \n   215→    key_points: List[str] = Field(\n   216→        default_factory=list,\n   217→        description=\"Key points extracted from the document\"\n   218→    )\n   219→    \n   220→    entities: List[str] = Field(\n   221→        default_factory=list,\n   222→        description=\"Important entities mentioned (people, organizations, etc.)\"\n   223→    )\n   224→    \n   225→    dates: List[str] = Field(\n   226→        default_factory=list,\n   227→        description=\"Important dates mentioned\"\n   228→    )\n   229→    \n   230→    numbers: List[str] = Field(\n   231→        default_factory=list,\n   232→        description=\"Important numbers or statistics mentioned\"\n   233→    )\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ]
  }
]