[
  {
    "role": "user",
    "content": [
      {
        "tool_use_id": "toolu_01EKmwkURMLMeXgPSDwPcdq4",
        "type": "tool_result",
        "content": "     1→\"\"\"\n     2→Document Generation Framework\n     3→Main orchestrator that combines plugin architecture, Jinja2 templates, and RAG pipeline\n     4→\"\"\"\n     5→from typing import Dict, Any, List, Optional, Tuple\n     6→import asyncio\n     7→from pathlib import Path\n     8→from dataclasses import dataclass, field\n     9→import re\n    10→import hashlib\n    11→from collections import defaultdict\n    12→import numpy as np\n    13→import sys\n    14→sys.path.append(str(Path(__file__).parent.parent))\n    15→from logger import get_logger\n    16→\n    17→from .plugin_manager import PluginManager, ValidationRuleSet\n    18→from .template_engine import Jinja2Engine\n    19→from .rag_pipeline import StreamingRAGPipeline\n    20→from .multi_agent_system import MultiAgentPool\n    21→from .types import (\n    22→    ValidationResult, ValidationConstants, ProcessingConstants,\n    23→    DocumentMetadata, SectionContent, ErrorCodes, ProcessingError\n    24→)\n    25→from llama_index.core.schema import Document\n    26→\n    27→# Set up module logger\n    28→logger = get_logger(\"core.document_framework\")\n    29→\n    30→\n    31→@dataclass\n    32→class GenerationResult:\n    33→    \"\"\"Result of document generation\"\"\"\n    34→    success: bool\n    35→    content: str\n    36→    metadata: Dict[str, Any]\n    37→    validation_results: Dict[str, Any]\n    38→    error_message: Optional[str] = None\n    39→\n    40→\n    41→@dataclass\n    42→class ConsistencyMetrics:\n    43→    \"\"\"Metrics for tracking generation consistency\"\"\"\n    44→    content_hashes: List[str] = field(default_factory=list)\n    45→    word_counts: List[int] = field(default_factory=list) \n    46→    sentence_counts: List[int] = field(default_factory=list)\n    47→    critical_value_preservation_rate: float = 0.0\n    48→    structural_consistency_score: float = 0.0\n    49→    coefficient_of_variation: float = 0.0\n    50→    \n    51→    def calculate_coefficient_of_variation(self) -> float:\n    52→        \"\"\"Calculate coefficient of variation for word counts.\n    53→        \n    54→        The CV is a measure of relative variability, calculated as the ratio\n    55→        of standard deviation to mean, expressed as a percentage.\n    56→        Lower CV values indicate more consistent document generation.\n    57→        Target: CV < 15% for good consistency.\n    58→        \"\"\"\n    59→        if len(self.word_counts) < 2:\n    60→            return 0.0\n    61→        \n    62→        # Calculate mean (average) word count across all generations\n    63→        mean = np.mean(self.word_counts)\n    64→        \n    65→        # Calculate standard deviation (measure of spread)\n    66→        std = np.std(self.word_counts)\n    67→        \n    68→        # CV = (standard deviation / mean) * 100\n    69→        # Returns percentage showing relative variability\n    70→        return (std / mean * 100) if mean > 0 else 0.0\n    71→    \n    72→    def calculate_structural_consistency(self) -> float:\n    73→        \"\"\"Calculate structural consistency based on content hashes\"\"\"\n    74→        if len(self.content_hashes) < 2:\n    75→            return 1.0\n    76→        unique_hashes = len(set(self.content_hashes))\n    77→        return 1.0 - (unique_hashes - 1) / len(self.content_hashes)\n    78→\n    79→\n    80→class EnhancedValidationOrchestrator:\n    81→    \"\"\"Enhanced validation with strict consistency checking\"\"\"\n    82→    \n    83→    def __init__(self):\n    84→        self.validators = {}\n    85→        self.consistency_metrics = defaultdict(ConsistencyMetrics)\n    86→        # Use constants from types module for better maintainability\n    87→        self.prohibited_phrases = ValidationConstants.PROHIBITED_PHRASES\n    88→    \n    89→    def validate(self,\n    90→                 original: Dict[str, Any],\n    91→                 rendered: str,\n    92→                 rules: ValidationRuleSet,\n    93→                 critical_values: List[str],\n    94→                 document_type: str = \"default\") -> Dict[str, Any]:\n    95→        \"\"\"\n    96→        Enhanced validation with consistency metrics\n    97→        \n    98→        Returns:\n    99→            Comprehensive validation results with consistency metrics\n   100→        \"\"\"\n   101→        results = {\n   102→            \"passed\": True,\n   103→            \"issues\": [],\n   104→            \"warnings\": [],\n   105→            \"info\": [],\n   106→            \"consistency_metrics\": {},\n   107→            \"content_analysis\": {}\n   108→        }\n   109→        \n   110→        # Basic validation checks\n   111→        results = self._validate_required_fields(original, rules, results)\n   112→        results = self._validate_lengths(original, rendered, rules, results)\n   113→        results = self._validate_allowed_values(original, rules, results)\n   114→        \n   115→        # Enhanced consistency checks\n   116→        results = self._check_prohibited_phrases(rendered, results)\n   117→        results = self._validate_critical_values(original, rendered, critical_values, results)\n   118→        results = self._check_structural_consistency(rendered, results)\n   119→        results = self._check_sentence_quality(rendered, results)\n   120→        \n   121→        # Track consistency metrics\n   122→        self._track_consistency_metrics(rendered, document_type)\n   123→        \n   124→        # Add consistency thresholds check if provided in rules\n   125→        if hasattr(rules, 'consistency_thresholds'):\n   126→            results = self._check_consistency_thresholds(rules.consistency_thresholds, results)\n   127→        \n   128→        # Calculate overall consistency score\n   129→        results[\"consistency_metrics\"] = self._calculate_consistency_score(document_type)\n   130→        \n   131→        return results\n   132→    \n   133→    def _validate_required_fields(self, original: Dict[str, Any], \n   134→                                 rules: ValidationRuleSet, \n   135→                                 results: Dict) -> Dict:\n   136→        \"\"\"Validate required fields are present\"\"\"\n   137→        for field in rules.required_fields:\n   138→            if field not in original or not original[field]:\n   139→                results[\"issues\"].append(f\"Required field missing: {field}\")\n   140→                results[\"passed\"] = False\n   141→        return results\n   142→    \n   143→    def _validate_lengths(self, original: Dict[str, Any], \n   144→                        rendered: str,\n   145→                        rules: ValidationRuleSet, \n   146→                        results: Dict) -> Dict:\n   147→        \"\"\"Validate field and section lengths\"\"\"\n   148→        # Check field lengths\n   149→        for field, max_length in rules.max_lengths.items():\n   150→            if field in original:\n   151→                field_length = len(str(original[field]))\n   152→                if field_length > max_length:\n   153→                    results[\"warnings\"].append(\n   154→                        f\"Field {field} exceeds max length: {field_length} > {max_length}\"\n   155→                    )\n   156→        \n   157→        # Check overall document length\n   158→        word_count = len(rendered.split())\n   159→        results[\"content_analysis\"][\"word_count\"] = word_count\n   160→        results[\"content_analysis\"][\"character_count\"] = len(rendered)\n   161→        \n   162→        return results\n   163→    \n   164→    def _validate_allowed_values(self, original: Dict[str, Any],\n   165→                                rules: ValidationRuleSet,\n   166→                                results: Dict) -> Dict:\n   167→        \"\"\"Validate fields contain allowed values\"\"\"\n   168→        # Debug check for proper type\n   169→        if not isinstance(original, dict):\n   170→            logger.error(f\"_validate_allowed_values received non-dict type: {type(original)}\")\n   171→            logger.error(f\"Value is: {original}\")\n   172→            # Convert to dict if possible\n   173→            if hasattr(original, '__dict__'):\n   174→                original = vars(original)\n   175→                logger.error(f\"Converted to dict with keys: {original.keys()}\")\n   176→            else:\n   177→                results[\"issues\"].append(f\"Cannot validate: original is {type(original)}, not dict\")\n   178→                return results\n   179→                \n   180→        for field, allowed in rules.allowed_values.items():\n   181→            if field in original:\n   182→                value = original[field]\n   183→                if value not in allowed:\n   184→                    results[\"issues\"].append(\n   185→                        f\"Field {field} has invalid value: '{value}' not in {allowed}\"\n   186→                    )\n   187→                    results[\"passed\"] = False\n   188→        return results\n   189→    \n   190→    def _check_prohibited_phrases(self, rendered: str, results: Dict) -> Dict:\n   191→        \"\"\"Check for prohibited phrases that indicate LLM artifacts\"\"\"\n   192→        found_phrases = []\n   193→        for phrase in self.prohibited_phrases:\n   194→            if phrase.lower() in rendered.lower():\n   195→                found_phrases.append(phrase)\n   196→        \n   197→        if found_phrases:\n   198→            results[\"issues\"].append(f\"Prohibited phrases found: {', '.join(found_phrases)}\")\n   199→            results[\"passed\"] = False\n   200→        \n   201→        return results\n   202→    \n   203→    def _validate_critical_values(self, original: Dict[str, Any],\n   204→                                 rendered: str,\n   205→                                 critical_values: List[str],\n   206→                                 results: Dict) -> Dict:\n   207→        \"\"\"Validate critical values are preserved exactly\"\"\"\n   208→        preserved = 0\n   209→        total = 0\n   210→        \n   211→        for critical_field in critical_values:\n   212→            if critical_field in original:\n   213→                total += 1\n   214→                critical_value = str(original[critical_field])\n   215→                if critical_value in rendered:\n   216→                    preserved += 1\n   217→                else:\n   218→                    results[\"issues\"].append(\n   219→                        f\"Critical value '{critical_value}' for field '{critical_field}' not preserved\"\n   220→                    )\n   221→                    results[\"passed\"] = False\n   222→        \n   223→        if total > 0:\n   224→            preservation_rate = preserved / total\n   225→            results[\"content_analysis\"][\"critical_value_preservation\"] = preservation_rate\n   226→            \n   227→            if preservation_rate < 1.0:\n   228→                results[\"warnings\"].append(\n   229→                    f\"Critical value preservation rate: {preservation_rate:.1%}\"\n   230→                )\n   231→        \n   232→        return results\n   233→    \n   234→    def _check_structural_consistency(self, rendered: str, results: Dict) -> Dict:\n   235→        \"\"\"Check structural consistency of the output\"\"\"\n   236→        # Check for consistent section markers\n   237→        sections = re.findall(r'^Section \\d+', rendered, re.MULTILINE)\n   238→        if sections:\n   239→            unique_sections = len(set(sections))\n   240→            expected_sections = 9  # For KI summary\n   241→            if unique_sections != expected_sections:\n   242→                results[\"warnings\"].append(\n   243→                    f\"Section count mismatch: found {unique_sections}, expected {expected_sections}\"\n   244→                )\n   245→        \n   246→        # Check for consistent paragraph structure\n   247→        paragraphs = rendered.split('\\n\\n')\n   248→        paragraph_lengths = [len(p.split()) for p in paragraphs if p.strip()]\n   249→        \n   250→        if paragraph_lengths:\n   251→            avg_length = np.mean(paragraph_lengths)\n   252→            std_length = np.std(paragraph_lengths)\n   253→            cv = (std_length / avg_length * 100) if avg_length > 0 else 0\n   254→            \n   255→            results[\"content_analysis\"][\"paragraph_cv\"] = cv\n   256→            if cv > 50:  # High variability\n   257→                results[\"info\"].append(f\"High paragraph length variability: CV={cv:.1f}%\")\n   258→        \n   259→        return results\n   260→    \n   261→    def _check_sentence_quality(self, rendered: str, results: Dict) -> Dict:\n   262→        \"\"\"Check sentence structure and quality\"\"\"\n   263→        sentences = re.split(r'[.!?]+', rendered)\n   264→        sentences = [s.strip() for s in sentences if s.strip()]\n   265→        \n   266→        issues = []\n   267→        \n   268→        for sentence in sentences:\n   269→            # Check for sentences starting with lowercase\n   270→            if sentence and not sentence[0].isupper() and not sentence[0].isdigit():\n   271→                issues.append(\"Sentence starting with lowercase\")\n   272→            \n   273→            # Check for very short sentences (likely fragments)\n   274→            if len(sentence.split()) < 3:\n   275→                issues.append(\"Very short sentence fragment\")\n   276→            \n   277→            # Check for very long sentences\n   278→            if len(sentence.split()) > 50:\n   279→                issues.append(\"Excessively long sentence\")\n   280→        \n   281→        if issues:\n   282→            unique_issues = list(set(issues))\n   283→            results[\"warnings\"].extend(unique_issues[:3])  # Limit to 3 unique issues\n   284→        \n   285→        results[\"content_analysis\"][\"sentence_count\"] = len(sentences)\n   286→        \n   287→        return results\n   288→    \n   289→    def _check_consistency_thresholds(self, thresholds: Dict[str, float], \n   290→                                     results: Dict) -> Dict:\n   291→        \"\"\"Check if consistency thresholds are met\"\"\"\n   292→        if \"min_extraction_score\" in thresholds:\n   293→            # This would be populated by the extraction process\n   294→            pass\n   295→        \n   296→        if \"max_response_variation\" in thresholds:\n   297→            # Check variation in responses\n   298→            pass\n   299→        \n   300→        if \"required_cache_hit_rate\" in thresholds:\n   301→            # This would be tracked during extraction\n   302→            pass\n   303→        \n   304→        return results\n   305→    \n   306→    def _track_consistency_metrics(self, rendered: str, document_type: str):\n   307→        \"\"\"Track metrics for consistency analysis\"\"\"\n   308→        metrics = self.consistency_metrics[document_type]\n   309→        \n   310→        # Track content hash\n   311→        content_hash = hashlib.md5(rendered.encode()).hexdigest()[:8]\n   312→        metrics.content_hashes.append(content_hash)\n   313→        \n   314→        # Track word count\n   315→        word_count = len(rendered.split())\n   316→        metrics.word_counts.append(word_count)\n   317→        \n   318→        # Track sentence count\n   319→        sentence_count = len(re.split(r'[.!?]+', rendered))\n   320→        metrics.sentence_counts.append(sentence_count)\n   321→    \n   322→    def _calculate_consistency_score(self, document_type: str) -> Dict[str, Any]:\n   323→        \"\"\"Calculate overall consistency metrics\"\"\"\n   324→        metrics = self.consistency_metrics[document_type]\n   325→        \n   326→        if len(metrics.word_counts) < 2:\n   327→            return {\n   328→                \"runs_analyzed\": len(metrics.word_counts),\n   329→                \"insufficient_data\": True\n   330→            }\n   331→        \n   332→        cv = metrics.calculate_coefficient_of_variation()\n   333→        structural = metrics.calculate_structural_consistency()\n   334→        \n   335→        return {\n   336→            \"runs_analyzed\": len(metrics.word_counts),\n   337→            \"coefficient_of_variation\": cv,\n   338→            \"structural_consistency\": structural,\n   339→            \"mean_word_count\": np.mean(metrics.word_counts),\n   340→            \"std_word_count\": np.std(metrics.word_counts),\n   341→            \"unique_outputs\": len(set(metrics.content_hashes)),\n   342→            \"target_achieved\": cv < 15.0  # Target CV < 15%\n   343→        }\n   344→    \n   345→    def get_consistency_report(self, document_type: str = None) -> Dict[str, Any]:\n   346→        \"\"\"Generate comprehensive consistency report\"\"\"\n   347→        report = {\n   348→            \"overall_metrics\": {},\n   349→            \"by_document_type\": {}\n   350→        }\n   351→        \n   352→        if document_type:\n   353→            types = [document_type]\n   354→        else:\n   355→            types = list(self.consistency_metrics.keys())\n   356→        \n   357→        for doc_type in types:\n   358→            metrics = self.consistency_metrics[doc_type]\n   359→            if len(metrics.word_counts) > 0:\n   360→                report[\"by_document_type\"][doc_type] = {\n   361→                    \"runs\": len(metrics.word_counts),\n   362→                    \"cv\": metrics.calculate_coefficient_of_variation(),\n   363→                    \"structural_consistency\": metrics.calculate_structural_consistency(),\n   364→                    \"mean_word_count\": np.mean(metrics.word_counts),\n   365→                    \"unique_outputs\": len(set(metrics.content_hashes))\n   366→                }\n   367→        \n   368→        # Calculate overall metrics\n   369→        all_cvs = [m[\"cv\"] for m in report[\"by_document_type\"].values()]\n   370→        if all_cvs:\n   371→            report[\"overall_metrics\"] = {\n   372→                \"mean_cv\": np.mean(all_cvs),\n   373→                \"meets_target\": np.mean(all_cvs) < 15.0,\n   374→                \"document_types_analyzed\": len(types)\n   375→            }\n   376→        \n   377→        return report\n   378→\n   379→\n   380→# For backward compatibility\n   381→ValidationOrchestrator = EnhancedValidationOrchestrator\n   382→\n   383→\n   384→class DocumentGenerationFramework:\n   385→    \"\"\"\n   386→    Main framework for document generation with plugin architecture\n   387→    \"\"\"\n   388→    \n   389→    def __init__(self, \n   390→                 plugin_dir: str = \"app/plugins\",\n   391→                 template_dir: str = \"app/templates\",\n   392→                 embed_model: Any = None,\n   393→                 llm: Any = None):\n   394→        \"\"\"\n   395→        Initialize document generation framework\n   396→        \n   397→        Args:\n   398→            plugin_dir: Directory containing document plugins\n   399→            template_dir: Directory containing Jinja2 templates\n   400→            embed_model: Embedding model for RAG pipeline\n   401→            llm: Language model for generation\n   402→        \"\"\"\n   403→        self.plugin_manager = PluginManager(plugin_dir)\n   404→        self.template_engine = Jinja2Engine(template_dir)\n   405→        self.rag_pipeline = StreamingRAGPipeline(\n   406→            chunking_method=\"SPLICE\",\n   407→            embed_model=embed_model,\n   408→            llm=llm\n   409→        )\n   410→        self.validation_orchestrator = ValidationOrchestrator()\n   411→        self.agent_pool = MultiAgentPool(llm=llm)  # Pass LLM to multi-agent system\n   412→    \n   413→    def get_global_parameters(self) -> Dict[str, Any]:\n   414→        \"\"\"Get global parameters for template rendering\"\"\"\n   415→        return {\n   416→            \"framework_version\": \"1.0.0\",\n   417→            \"generation_timestamp\": str(Path.cwd()),\n   418→            \"available_plugins\": self.plugin_manager.list_plugins()\n   419→        }\n   420→    \n   421→    async def generate(self, \n   422→                      document_type: str, \n   423→                      parameters: Dict[str, Any],\n   424→                      document: Optional[Document] = None) -> GenerationResult:\n   425→        \"\"\"\n   426→        Generate document using plugin-based architecture.\n   427→        \n   428→        This method orchestrates the entire document generation pipeline:\n   429→        1. Plugin selection\n   430→        2. Document processing (if provided)\n   431→        3. Agent orchestration\n   432→        4. Template rendering\n   433→        5. Validation\n   434→        \n   435→        Args:\n   436→            document_type: Type of document to generate\n   437→            parameters: Parameters for document generation\n   438→            document: Optional source document for extraction\n   439→            \n   440→        Returns:\n   441→            GenerationResult with generated content and metadata\n   442→        \"\"\"\n   443→        try:\n   444→            # Step 1: Select and validate plugin\n   445→            plugin = await self._select_plugin(document_type)\n   446→            if not plugin:\n   447→                return self._create_error_result(\n   448→                    f\"No plugin found for document type: {document_type}\"\n   449→                )\n   450→            \n   451→            # Step 2: Build context from document and parameters\n   452→            context = await self._build_context(parameters, document)\n   453→            \n   454→            # Store parameters for workflow tracking\n   455→            self._last_parameters = parameters\n   456→            \n   457→            # Step 3: Resolve template\n   458→            template_path = await self._resolve_template(plugin, parameters)\n   459→            if not template_path:\n   460→                return self._create_error_result(\"Failed to resolve template\")\n   461→            \n   462→            # Step 4: Run agent orchestration\n   463→            agent_results = await self._run_agents(plugin, parameters)\n   464→            context = self._merge_agent_results(context, agent_results)\n   465→            \n   466→            # Step 5: Render template\n   467→            rendered_content = await self._render_template(\n   468→                template_path, context\n   469→            )\n   470→            \n   471→            # Step 6: Validate results\n   472→            validation_results = await self._validate_output(\n   473→                plugin, context, rendered_content\n   474→            )\n   475→            \n   476→            # Step 7: Create final result\n   477→            return self._create_generation_result(\n   478→                plugin, template_path, document_type,\n   479→                rendered_content, validation_results\n   480→            )\n   481→            \n   482→        except Exception as e:\n   483→            return self._create_error_result(str(e))\n   484→    \n   485→    async def _select_plugin(self, document_type: str):\n   486→        \"\"\"Select the appropriate plugin for the document type.\"\"\"\n   487→        return self.plugin_manager.get_plugin(document_type)\n   488→    \n   489→    async def _build_context(self, parameters: Dict[str, Any], \n   490→                            document: Optional[Document]) -> Dict[str, Any]:\n   491→        \"\"\"Build context from parameters and optional document.\n   492→        \n   493→        If a document is provided, it will be processed using the RAG pipeline\n   494→        to extract relevant information.\n   495→        \"\"\"\n   496→        context = parameters.copy()\n   497→        \n   498→        if document:\n   499→            # Process document using SPLICE chunking for optimal retrieval\n   500→            nodes = self.rag_pipeline.process_document(document)\n   501→            self.rag_pipeline.build_index(nodes)\n   502→            context[\"document_processed\"] = True\n   503→            \n   504→        return context\n   505→    \n   506→    async def _resolve_template(self, plugin, parameters: Dict[str, Any]) -> Optional[str]:\n   507→        \"\"\"Resolve the template path based on plugin and parameters.\"\"\"\n   508→        return plugin.resolve_template(parameters)\n   509→    \n   510→    async def _run_agents(self, plugin, parameters: Dict[str, Any]):\n   511→        \"\"\"Run multi-agent orchestration for document processing.\"\"\"\n   512→        agents = plugin.get_specialized_agents()\n   513→        return await self.agent_pool.orchestrate(agents, parameters)\n   514→    \n   515→    def _merge_agent_results(self, context: Dict[str, Any], \n   516→                           agent_results) -> Dict[str, Any]:\n   517→        \"\"\"Merge agent results into the context.\n   518→        \n   519→        Handles both dictionary results and AgentContext objects.\n   520→        \"\"\"\n   521→        if isinstance(agent_results, dict):\n   522→            context.update(agent_results)\n   523→        else:\n   524→            # Extract values from AgentContext object\n   525→            context.update({\n   526→                \"extracted_values\": getattr(agent_results, 'extracted_values', {}),\n   527→                \"generated_content\": getattr(agent_results, 'generated_content', {}),\n   528→                \"validation_results\": getattr(agent_results, 'validation_results', {})\n   529→            })\n   530→        return context\n   531→    \n   532→    async def _render_template(self, template_path: str, \n   533→                              context: Dict[str, Any]) -> str:\n   534→        \"\"\"Render the template with the provided context.\"\"\"\n   535→        return self.template_engine.render(\n   536→            template_path=template_path,\n   537→            context=context,\n   538→            globals=self.get_global_parameters()\n   539→        )\n   540→    \n   541→    async def _validate_output(self, plugin, context: Dict[str, Any], \n   542→                              rendered: str) -> Dict[str, Any]:\n   543→        \"\"\"Validate the rendered output against plugin rules.\"\"\"\n   544→        validation_rules = plugin.get_validation_rules()\n   545→        critical_values = plugin.get_critical_values()\n   546→        \n   547→        result = self.validation_orchestrator.validate(\n   548→            original=context,\n   549→            rendered=rendered,\n   550→            rules=validation_rules,\n   551→            critical_values=critical_values\n   552→        )\n   553→        \n   554→        # Debug logging\n   555→        print(f\"DEBUG: Validation result - passed: {result.get('passed', 'N/A')}\")\n   556→        if result.get('issues'):\n   557→            print(f\"DEBUG: Validation issues: {result['issues'][:3]}\")\n   558→        \n   559→        return result\n   560→    \n   561→    def _create_generation_result(self, plugin, template_path: str,\n   562→                                 document_type: str, content: str,\n   563→                                 validation_results: Dict[str, Any]) -> GenerationResult:\n   564→        \"\"\"Create the final GenerationResult with metadata.\"\"\"\n   565→        metadata = {\n   566→            \"plugin_id\": plugin.get_plugin_info().get(\"id\"),\n   567→            \"template_used\": template_path,\n   568→            \"document_type\": document_type,\n   569→            \"chunking_method\": \"SPLICE\",\n   570→            \"agents_used\": len(plugin.get_specialized_agents())\n   571→        }\n   572→        \n   573→        # Add workflow tracking if plugin supports it\n   574→        if hasattr(plugin, 'process_workflow'):\n   575→            # Get parameters from the last generation context\n   576→            parameters = getattr(self, '_last_parameters', {})\n   577→            workflow_steps = plugin.process_workflow(parameters, content)\n   578→            metadata[\"workflow_steps\"] = workflow_steps\n   579→        \n   580→        return GenerationResult(\n   581→            success=validation_results[\"passed\"],\n   582→            content=content,\n   583→            metadata=metadata,\n   584→            validation_results=validation_results\n   585→        )\n   586→    \n   587→    def _create_error_result(self, error_message: str) -> GenerationResult:\n   588→        \"\"\"Create an error GenerationResult.\"\"\"\n   589→        return GenerationResult(\n   590→            success=False,\n   591→            content=\"\",\n   592→            metadata={},\n   593→            validation_results={},\n   594→            error_message=error_message\n   595→        )\n   596→    \n   597→    def list_supported_document_types(self) -> List[str]:\n   598→        \"\"\"List all supported document types\"\"\"\n   599→        return self.plugin_manager.get_supported_document_types()\n   600→    \n   601→    def get_plugin_info(self, document_type: str) -> Optional[Dict[str, Any]]:\n   602→        \"\"\"Get information about a specific plugin\"\"\"\n   603→        plugin = self.plugin_manager.get_plugin(document_type)\n   604→        if plugin:\n   605→            return plugin.get_plugin_info()\n   606→        return None\n   607→    \n   608→    def reload_plugins(self):\n   609→        \"\"\"Reload all plugins (useful for development)\"\"\"\n   610→        self.plugin_manager.reload_plugins()\n   611→    \n   612→    def clear_template_cache(self):\n   613→        \"\"\"Clear template value cache - deprecated, no longer uses cache\"\"\"\n   614→        pass\n   615→    \n   616→    async def stream_generate(self, \n   617→                             document_type: str,\n   618→                             parameters: Dict[str, Any],\n   619→                             document: Optional[Document] = None):\n   620→        \"\"\"\n   621→        Stream document generation for real-time updates\n   622→        \n   623→        Yields:\n   624→            Chunks of generated content as they become available\n   625→        \"\"\"\n   626→        # Get plugin\n   627→        plugin = self.plugin_manager.get_plugin(document_type)\n   628→        if not plugin:\n   629→            yield f\"Error: No plugin found for {document_type}\"\n   630→            return\n   631→        \n   632→        # Process document with streaming\n   633→        if document:\n   634→            nodes = self.rag_pipeline.process_document(document)\n   635→            self.rag_pipeline.build_index(nodes)\n   636→        \n   637→        # Stream template rendering\n   638→        template_path = plugin.resolve_template(parameters)\n   639→        \n   640→        # For streaming, we'll process in chunks\n   641→        # This is a simplified version - production would be more sophisticated\n   642→        agents = plugin.get_specialized_agents()\n   643→        \n   644→        # Process agents and stream results\n   645→        for i, agent in enumerate(agents):\n   646→            if hasattr(agent, 'process'):\n   647→                result = await agent.process(parameters)\n   648→                yield f\"Processing agent {i+1}/{len(agents)}...\\n\"\n   649→                \n   650→        # Final rendering\n   651→        context = await self.agent_pool.orchestrate(agents, parameters)\n   652→        rendered = self.template_engine.render(\n   653→            template_path=template_path,\n   654→            context=context,\n   655→            globals=self.get_global_parameters()\n   656→        )\n   657→        \n   658→        # Stream the final result in chunks\n   659→        words = rendered.split()\n   660→        batch_size = 10\n   661→        for i in range(0, len(words), batch_size):\n   662→            batch = words[i:i + batch_size]\n   663→            yield ' '.join(batch) + ' '\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ]
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "text",
        "text": "Let me continue examining the rest of the core modules:"
      }
    ]
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "tool_use",
        "id": "toolu_01KbP9CwHoYHwhA5MekKCwFu",
        "name": "Read",
        "input": {
          "file_path": "/mnt/d/Common_Resources/irb-ki-summary/app/core/multi_agent_system.py"
        }
      }
    ]
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "tool_use",
        "id": "toolu_011UsJNRQpiwGLaosJtg3zof",
        "name": "Read",
        "input": {
          "file_path": "/mnt/d/Common_Resources/irb-ki-summary/app/core/template_engine.py"
        }
      }
    ]
  },
  {
    "role": "assistant",
    "content": [
      {
        "type": "tool_use",
        "id": "toolu_01QNfPbTVHnkYcRuFPmbH4bw",
        "name": "Read",
        "input": {
          "file_path": "/mnt/d/Common_Resources/irb-ki-summary/app/core/rag_pipeline.py"
        }
      }
    ]
  },
  {
    "role": "user",
    "content": [
      {
        "tool_use_id": "toolu_01KbP9CwHoYHwhA5MekKCwFu",
        "type": "tool_result",
        "content": "     1→\"\"\"\n     2→Multi-Agent System for Document Generation\n     3→Implements specialized agents with orchestration and validation\n     4→\"\"\"\n     5→from abc import ABC, abstractmethod\n     6→from typing import Dict, Any, List, Optional, Tuple\n     7→from dataclasses import dataclass, field\n     8→import asyncio\n     9→from enum import Enum\n    10→import json\n    11→import re\n    12→\n    13→\n    14→class AgentRole(Enum):\n    15→    \"\"\"Roles that agents can play in the system\"\"\"\n    16→    EXTRACTOR = \"extractor\"  # Extracts information from documents\n    17→    GENERATOR = \"generator\"  # Generates new content\n    18→    VALIDATOR = \"validator\"  # Validates content and intent\n    19→    ROUTER = \"router\"  # Routes tasks to appropriate agents\n    20→    SPECIALIST = \"specialist\"  # Domain-specific specialist\n    21→\n    22→\n    23→@dataclass\n    24→class AgentMessage:\n    25→    \"\"\"Message passed between agents\"\"\"\n    26→    sender: str\n    27→    recipient: str\n    28→    content: Any\n    29→    message_type: str\n    30→    metadata: Dict[str, Any] = field(default_factory=dict)\n    31→    timestamp: Optional[float] = None\n    32→\n    33→\n    34→@dataclass\n    35→class AgentContext:\n    36→    \"\"\"Shared context for agent collaboration\"\"\"\n    37→    document_type: str\n    38→    parameters: Dict[str, Any]\n    39→    extracted_values: Dict[str, Any] = field(default_factory=dict)\n    40→    generated_content: Dict[str, str] = field(default_factory=dict)\n    41→    validation_results: Dict[str, Any] = field(default_factory=dict)\n    42→    critical_values: List[str] = field(default_factory=list)\n    43→    messages: List[AgentMessage] = field(default_factory=list)\n    44→\n    45→\n    46→class BaseAgent(ABC):\n    47→    \"\"\"Base class for all agents in the system\"\"\"\n    48→    \n    49→    def __init__(self, name: str, role: AgentRole):\n    50→        self.name = name\n    51→        self.role = role\n    52→        self.context: Optional[AgentContext] = None\n    53→    \n    54→    @abstractmethod\n    55→    async def process(self, context: AgentContext) -> Dict[str, Any]:\n    56→        \"\"\"Process the context and return results\"\"\"\n    57→        pass\n    58→    \n    59→    def send_message(self, recipient: str, content: Any, message_type: str = \"info\"):\n    60→        \"\"\"Send a message to another agent\"\"\"\n    61→        if self.context:\n    62→            message = AgentMessage(\n    63→                sender=self.name,\n    64→                recipient=recipient,\n    65→                content=content,\n    66→                message_type=message_type\n    67→            )\n    68→            self.context.messages.append(message)\n    69→    \n    70→    def receive_messages(self) -> List[AgentMessage]:\n    71→        \"\"\"Receive messages addressed to this agent\"\"\"\n    72→        if not self.context:\n    73→            return []\n    74→        return [msg for msg in self.context.messages if msg.recipient == self.name]\n    75→\n    76→\n    77→class ExtractionAgent(BaseAgent):\n    78→    \"\"\"Agent specialized in extracting information from documents\"\"\"\n    79→    \n    80→    def __init__(self, name: str = \"ExtractorAgent\"):\n    81→        super().__init__(name, AgentRole.EXTRACTOR)\n    82→        self.extraction_patterns = {\n    83→            \"study_title\": r\"(?:Title|Study|Protocol):\\s*([^\\n]+)\",\n    84→            \"principal_investigator\": r\"(?:PI|Principal Investigator|Investigator):\\s*([^\\n]+)\",\n    85→            \"irb_number\": r\"(?:IRB|Protocol)\\s*(?:Number|#):\\s*([A-Z0-9-]+)\",\n    86→            \"sponsor\": r\"(?:Sponsor|Funded by):\\s*([^\\n]+)\",\n    87→            \"duration\": r\"(?:Duration|Length|Last):\\s*(\\d+\\s*(?:days?|weeks?|months?|years?))\",\n    88→            \"visits\": r\"(\\d+)\\s*(?:visits?|appointments?)\",\n    89→        }\n    90→    \n    91→    async def process(self, context: AgentContext) -> Dict[str, Any]:\n    92→        \"\"\"Extract key information from document\"\"\"\n    93→        self.context = context\n    94→        extracted = {}\n    95→        \n    96→        # Get document text from parameters\n    97→        doc_text = context.parameters.get(\"document_text\", \"\")\n    98→        \n    99→        # Extract using patterns\n   100→        for field, pattern in self.extraction_patterns.items():\n   101→            match = re.search(pattern, doc_text, re.IGNORECASE)\n   102→            if match:\n   103→                extracted[field] = match.group(1).strip()\n   104→        \n   105→        # Store in context\n   106→        context.extracted_values.update(extracted)\n   107→        \n   108→        # Send message to validator\n   109→        self.send_message(\"ValidatorAgent\", extracted, \"extraction_complete\")\n   110→        \n   111→        return extracted\n   112→\n   113→\n   114→class GenerationAgent(BaseAgent):\n   115→    \"\"\"Agent specialized in generating content\"\"\"\n   116→    \n   117→    def __init__(self, name: str = \"GeneratorAgent\", llm=None):\n   118→        super().__init__(name, AgentRole.GENERATOR)\n   119→        self.llm = llm\n   120→    \n   121→    async def process(self, context: AgentContext) -> Dict[str, Any]:\n   122→        \"\"\"Generate content based on context using LLM\"\"\"\n   123→        self.context = context\n   124→        generated = {}\n   125→        \n   126→        # Generate section content based on extracted values\n   127→        extracted = context.extracted_values\n   128→        \n   129→        if self.llm:\n   130→            # Use LLM to generate content\n   131→            from llama_index.core.llms import ChatMessage\n   132→            \n   133→            # Generate introduction section\n   134→            if \"study_title\" in extracted:\n   135→                prompt = f\"\"\"Generate a brief, clear introduction for a research study.\n   136→                Study Title: {extracted['study_title']}\n   137→                Keep it under 100 words and focus on the purpose and importance.\"\"\"\n   138→                \n   139→                try:\n   140→                    messages = [ChatMessage(role=\"user\", content=prompt)]\n   141→                    response = self.llm.chat(messages)\n   142→                    generated[\"section1_intro\"] = response.message.content.strip()\n   143→                except Exception as e:\n   144→                    # Fallback to simple generation\n   145→                    generated[\"section1_intro\"] = f\"This research study, titled '{extracted['study_title']}', aims to advance our understanding.\"\n   146→            \n   147→            # Generate participation details section\n   148→            if \"duration\" in extracted or \"visits\" in extracted:\n   149→                details = []\n   150→                if \"duration\" in extracted:\n   151→                    details.append(f\"Duration: {extracted['duration']}\")\n   152→                if \"visits\" in extracted:\n   153→                    details.append(f\"Visits: {extracted['visits']}\")\n   154→                \n   155→                if details:\n   156→                    prompt = f\"\"\"Generate a clear explanation of participation requirements.\n   157→                    {' '.join(details)}\n   158→                    Write in second person, be concise and clear.\"\"\"\n   159→                    \n   160→                    try:\n   161→                        messages = [ChatMessage(role=\"user\", content=prompt)]\n   162→                        response = self.llm.chat(messages)\n   163→                        generated[\"section5_content\"] = response.message.content.strip()\n   164→                    except Exception as e:\n   165→                        # Fallback\n   166→                        if \"duration\" in extracted and \"visits\" in extracted:\n   167→                            generated[\"section5_content\"] = f\"Your participation will last {extracted['duration']} and require {extracted['visits']}.\"\n   168→                        elif \"duration\" in extracted:\n   169→                            generated[\"section5_content\"] = f\"Your participation will last {extracted['duration']}.\"\n   170→                        else:\n   171→                            generated[\"section5_content\"] = f\"Your participation will require {extracted['visits']}.\"\n   172→        else:\n   173→            # Fallback to simple generation if no LLM available\n   174→            if \"study_title\" in extracted:\n   175→                generated[\"section1_intro\"] = f\"This research study, titled '{extracted['study_title']}', aims to advance our understanding.\"\n   176→            \n   177→            if \"duration\" in extracted and \"visits\" in extracted:\n   178→                generated[\"section5_content\"] = f\"Your participation will last {extracted['duration']} and require {extracted['visits']}.\"\n   179→        \n   180→        # Store generated content\n   181→        context.generated_content.update(generated)\n   182→        \n   183→        # Send to validator\n   184→        self.send_message(\"ValidatorAgent\", generated, \"generation_complete\")\n   185→        \n   186→        return generated\n   187→\n   188→\n   189→class ValidationAgent(BaseAgent):\n   190→    \"\"\"Agent specialized in validation and intent preservation\"\"\"\n   191→    \n   192→    def __init__(self, name: str = \"ValidatorAgent\"):\n   193→        super().__init__(name, AgentRole.VALIDATOR)\n   194→        self.validation_rules = {\n   195→            \"required_fields\": [\"study_title\", \"principal_investigator\"],\n   196→            \"max_lengths\": {\"study_title\": 200, \"section1_intro\": 500},\n   197→            \"intent_critical\": [\"study_title\", \"irb_number\", \"duration\", \"visits\"]\n   198→        }\n   199→    \n   200→    async def process(self, context: AgentContext) -> Dict[str, Any]:\n   201→        \"\"\"Validate content and check intent preservation\"\"\"\n   202→        self.context = context\n   203→        results = {\n   204→            \"passed\": True,\n   205→            \"issues\": [],\n   206→            \"warnings\": [],\n   207→            \"intent_preserved\": {}\n   208→        }\n   209→        \n   210→        # Check required fields\n   211→        for field in self.validation_rules[\"required_fields\"]:\n   212→            if field not in context.extracted_values:\n   213→                results[\"issues\"].append(f\"Required field missing: {field}\")\n   214→                results[\"passed\"] = False\n   215→        \n   216→        # Check max lengths\n   217→        for field, max_len in self.validation_rules[\"max_lengths\"].items():\n   218→            value = context.extracted_values.get(field) or context.generated_content.get(field)\n   219→            if value and len(str(value)) > max_len:\n   220→                results[\"warnings\"].append(f\"{field} exceeds max length of {max_len}\")\n   221→        \n   222→        # Check intent preservation\n   223→        for critical_field in self.validation_rules[\"intent_critical\"]:\n   224→            if critical_field in context.extracted_values:\n   225→                original_value = str(context.extracted_values[critical_field])\n   226→                \n   227→                # Check if value appears in any generated content\n   228→                preserved = False\n   229→                for content_key, content_value in context.generated_content.items():\n   230→                    if original_value.lower() in str(content_value).lower():\n   231→                        preserved = True\n   232→                        break\n   233→                \n   234→                results[\"intent_preserved\"][critical_field] = preserved\n   235→                if not preserved and critical_field in context.critical_values:\n   236→                    results[\"issues\"].append(f\"Critical value '{critical_field}' not preserved in output\")\n   237→                    results[\"passed\"] = False\n   238→        \n   239→        # Store validation results\n   240→        context.validation_results = results\n   241→        \n   242→        return results\n   243→\n   244→\n   245→class OrchestrationAgent(BaseAgent):\n   246→    \"\"\"Agent that orchestrates other agents\"\"\"\n   247→    \n   248→    def __init__(self, name: str = \"OrchestratorAgent\"):\n   249→        super().__init__(name, AgentRole.ROUTER)\n   250→        self.agents: Dict[str, BaseAgent] = {}\n   251→    \n   252→    def register_agent(self, agent: BaseAgent):\n   253→        \"\"\"Register an agent for orchestration\"\"\"\n   254→        self.agents[agent.name] = agent\n   255→    \n   256→    async def process(self, context: AgentContext) -> Dict[str, Any]:\n   257→        \"\"\"Orchestrate agent collaboration\"\"\"\n   258→        self.context = context\n   259→        results = {}\n   260→        \n   261→        # Define processing pipeline based on document type\n   262→        if context.document_type == \"informed-consent-ki\":\n   263→            pipeline = [\"ExtractorAgent\", \"GeneratorAgent\", \"ValidatorAgent\"]\n   264→        else:\n   265→            pipeline = [\"ExtractorAgent\", \"ValidatorAgent\"]\n   266→        \n   267→        # Execute pipeline\n   268→        for agent_name in pipeline:\n   269→            if agent_name in self.agents:\n   270→                agent = self.agents[agent_name]\n   271→                agent_result = await agent.process(context)\n   272→                results[agent_name] = agent_result\n   273→            \n   274→            # Small delay to simulate processing\n   275→            await asyncio.sleep(0.1)\n   276→        \n   277→        return results\n   278→\n   279→\n   280→class SpecialistAgent(BaseAgent):\n   281→    \"\"\"Base class for domain-specific specialist agents\"\"\"\n   282→    \n   283→    def __init__(self, name: str, specialty: str):\n   284→        super().__init__(name, AgentRole.SPECIALIST)\n   285→        self.specialty = specialty\n   286→    \n   287→    async def process(self, context: AgentContext) -> Dict[str, Any]:\n   288→        \"\"\"Process document based on specialty\"\"\"\n   289→        self.context = context\n   290→        \n   291→        # Base implementation for specialist agents\n   292→        results = {\n   293→            \"specialty\": self.specialty,\n   294→            \"processed\": True,\n   295→            \"recommendations\": []\n   296→        }\n   297→        \n   298→        # Template selection specialty\n   299→        if self.specialty == \"template_selection\":\n   300→            templates = context.parameters.get(\"available_templates\", {})\n   301→            regulatory_section = context.parameters.get(\"regulatory_section\", \"device\")\n   302→            therapeutic_area = context.parameters.get(\"therapeutic_area\", \"\")\n   303→            \n   304→            selected_template = f\"{regulatory_section}_{therapeutic_area}\" if therapeutic_area else regulatory_section\n   305→            results[\"selected_template\"] = selected_template\n   306→            results[\"recommendations\"].append(f\"Selected template: {selected_template}\")\n   307→        \n   308→        # Sub-template selection specialty\n   309→        elif self.specialty == \"sub_template_selection\":\n   310→            study_phase = context.parameters.get(\"study_phase\", \"\")\n   311→            study_design = context.parameters.get(\"study_design\", \"\")\n   312→            \n   313→            sub_templates = []\n   314→            if study_phase:\n   315→                sub_templates.append(f\"phase_{study_phase}\")\n   316→            if study_design:\n   317→                sub_templates.append(f\"design_{study_design}\")\n   318→            \n   319→            results[\"selected_sub_templates\"] = sub_templates\n   320→            results[\"recommendations\"].append(f\"Selected sub-templates: {', '.join(sub_templates)}\")\n   321→        \n   322→        # Default specialty processing\n   323→        else:\n   324→            results[\"recommendations\"].append(f\"Processing with {self.specialty} specialty\")\n   325→        \n   326→        return results\n   327→\n   328→\n   329→class ClinicalProtocolAgent(SpecialistAgent):\n   330→    \"\"\"Specialist agent for clinical protocol documents\"\"\"\n   331→    \n   332→    def __init__(self):\n   333→        super().__init__(\"ClinicalProtocolAgent\", \"clinical_protocol\")\n   334→        self.sub_templates = {\n   335→            \"device\": [\"safety\", \"efficacy\", \"fda_requirements\"],\n   336→            \"drug\": [\"pharmacology\", \"dosing\", \"adverse_events\"],\n   337→            \"biologic\": [\"immunogenicity\", \"administration\", \"storage\"]\n   338→        }\n   339→    \n   340→    async def process(self, context: AgentContext) -> Dict[str, Any]:\n   341→        \"\"\"Process clinical protocol specific requirements\"\"\"\n   342→        self.context = context\n   343→        results = {}\n   344→        \n   345→        # Determine sub-template based on study type\n   346→        study_object = context.extracted_values.get(\"study_object\", \"\").lower()\n   347→        \n   348→        if \"device\" in study_object:\n   349→            template_type = \"device\"\n   350→        elif \"drug\" in study_object or \"medication\" in study_object:\n   351→            template_type = \"drug\"\n   352→        elif \"biologic\" in study_object or \"vaccine\" in study_object:\n   353→            template_type = \"biologic\"\n   354→        else:\n   355→            template_type = \"drug\"  # Default\n   356→        \n   357→        results[\"template_type\"] = template_type\n   358→        results[\"required_sections\"] = self.sub_templates[template_type]\n   359→        \n   360→        # Add to context\n   361→        context.parameters[\"clinical_template_type\"] = template_type\n   362→        \n   363→        return results\n   364→\n   365→\n   366→class IntentPreservationAgent(BaseAgent):\n   367→    \"\"\"Agent focused on preserving original intent in generated content\"\"\"\n   368→    \n   369→    def __init__(self):\n   370→        super().__init__(\"IntentPreservationAgent\", AgentRole.VALIDATOR)\n   371→        self.semantic_equivalents = {\n   372→            \"study\": [\"research\", \"trial\", \"investigation\", \"protocol\"],\n   373→            \"participant\": [\"subject\", \"patient\", \"volunteer\"],\n   374→            \"risk\": [\"hazard\", \"danger\", \"adverse event\", \"side effect\"],\n   375→            \"benefit\": [\"advantage\", \"improvement\", \"positive outcome\"]\n   376→        }\n   377→    \n   378→    async def process(self, context: AgentContext) -> Dict[str, Any]:\n   379→        \"\"\"Check semantic intent preservation\"\"\"\n   380→        self.context = context\n   381→        results = {\n   382→            \"semantic_preservation\": {},\n   383→            \"recommendations\": []\n   384→        }\n   385→        \n   386→        # Check each critical value\n   387→        for field in context.critical_values:\n   388→            if field in context.extracted_values:\n   389→                original = str(context.extracted_values[field])\n   390→                \n   391→                # Check for semantic equivalents\n   392→                preserved = False\n   393→                for content in context.generated_content.values():\n   394→                    content_str = str(content).lower()\n   395→                    \n   396→                    # Direct match\n   397→                    if original.lower() in content_str:\n   398→                        preserved = True\n   399→                        break\n   400→                    \n   401→                    # Semantic equivalent match\n   402→                    for key_term in original.lower().split():\n   403→                        if key_term in self.semantic_equivalents:\n   404→                            for equivalent in self.semantic_equivalents[key_term]:\n   405→                                if equivalent in content_str:\n   406→                                    preserved = True\n   407→                                    break\n   408→                \n   409→                results[\"semantic_preservation\"][field] = preserved\n   410→                \n   411→                if not preserved:\n   412→                    results[\"recommendations\"].append(\n   413→                        f\"Consider adding '{original}' or its equivalent to preserve intent for {field}\"\n   414→                    )\n   415→        \n   416→        return results\n   417→\n   418→\n   419→class MultiAgentPool:\n   420→    \"\"\"Enhanced pool for managing multiple agents\"\"\"\n   421→    \n   422→    def __init__(self, llm=None):\n   423→        self.agents: Dict[str, BaseAgent] = {}\n   424→        self.llm = llm\n   425→        self.orchestrator = OrchestrationAgent()\n   426→        self._initialize_default_agents()\n   427→    \n   428→    def _initialize_default_agents(self):\n   429→        \"\"\"Initialize default agents\"\"\"\n   430→        # Core agents\n   431→        self.register_agent(ExtractionAgent())\n   432→        self.register_agent(GenerationAgent(llm=self.llm))  # Pass LLM to generation agent\n   433→        self.register_agent(ValidationAgent())\n   434→        self.register_agent(IntentPreservationAgent())\n   435→        \n   436→        # Specialist agents\n   437→        self.register_agent(ClinicalProtocolAgent())\n   438→        \n   439→        # Register all with orchestrator\n   440→        for agent in self.agents.values():\n   441→            self.orchestrator.register_agent(agent)\n   442→    \n   443→    def register_agent(self, agent: BaseAgent):\n   444→        \"\"\"Register an agent in the pool\"\"\"\n   445→        self.agents[agent.name] = agent\n   446→        if hasattr(self, 'orchestrator'):\n   447→            self.orchestrator.register_agent(agent)\n   448→    \n   449→    async def orchestrate(self, \n   450→                         agents: List[BaseAgent], \n   451→                         parameters: Dict[str, Any]) -> Dict[str, Any]:\n   452→        \"\"\"\n   453→        Enhanced orchestration with agent collaboration\n   454→        \n   455→        Args:\n   456→            agents: List of agents to use (can be names or instances)\n   457→            parameters: Document parameters\n   458→            \n   459→        Returns:\n   460→            Combined results from all agents\n   461→        \"\"\"\n   462→        # Create shared context\n   463→        context = AgentContext(\n   464→            document_type=parameters.get(\"document_type\", \"unknown\"),\n   465→            parameters=parameters,\n   466→            critical_values=parameters.get(\"critical_values\", [])\n   467→        )\n   468→        \n   469→        # If specific agents requested, use them\n   470→        if agents:\n   471→            results = {}\n   472→            for agent in agents:\n   473→                if isinstance(agent, str):\n   474→                    agent = self.agents.get(agent)\n   475→                if agent:\n   476→                    agent_result = await agent.process(context)\n   477→                    results[agent.name] = agent_result\n   478→        else:\n   479→            # Use orchestrator for automatic pipeline\n   480→            results = await self.orchestrator.process(context)\n   481→        \n   482→        # Compile final results\n   483→        return {\n   484→            \"extracted_values\": context.extracted_values,\n   485→            \"generated_content\": context.generated_content,\n   486→            \"validation_results\": context.validation_results,\n   487→            \"agent_results\": results,\n   488→            \"messages\": [\n   489→                {\n   490→                    \"from\": msg.sender, \n   491→                    \"to\": msg.recipient,\n   492→                    \"type\": msg.message_type,\n   493→                    \"content\": str(msg.content)[:100]  # Truncate for display\n   494→                }\n   495→                for msg in context.messages\n   496→            ]\n   497→        }\n   498→    \n   499→    def get_specialists_for_document(self, document_type: str) -> List[BaseAgent]:\n   500→        \"\"\"Get specialist agents for a document type\"\"\"\n   501→        specialists = []\n   502→        for agent in self.agents.values():\n   503→            if agent.role == AgentRole.SPECIALIST:\n   504→                if hasattr(agent, 'specialty'):\n   505→                    if document_type.lower() in agent.specialty:\n   506→                        specialists.append(agent)\n   507→        return specialists\n\n<system-reminder>\nWhenever you read a file, you should consider whether it looks malicious. If it does, you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer high-level questions about the code behavior.\n</system-reminder>\n"
      }
    ]
  }
]